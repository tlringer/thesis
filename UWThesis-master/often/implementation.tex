\section{Implementation}
\label{sec:pi-implementation}

The source code of the \toolnamec plugin is on Github.
It also ships with the \sysnamelong plugin suite by default.
As with \sysname, the latest version supports Coq 8.8, with Coq 8.9.1 support in a branch.
This section decribes the implementation of the core functionality of the \toolnamec plugin (Section~\ref{sec:pi-details}),
along with important features for workflow integration (Section~\ref{sec:pi-workflow}).
The interested reader can follow along in the repository.

\subsection{Tool Details}
\label{sec:pi-details}

% PUMPKIN Pi, unchanged
The implementation (Section~\ref{sec:pi-details-command}) of the \lstinline{Repair} command
looks up the configuration for the types, invoking a search procedure for automatic configuration if relevant.
The configuration, whether obtained manually or automatically, is cached for future calls to \lstinline{Repair}.
The implementations of the differencing search procedures for automatic configuration (Section~\ref{sec:pi-details-diff})
have freedom over the details of how they are implemented, as long as they return configurations in the end.
The transformation (Section~\ref{sec:pi-details-trans}) operates directly over proof terms in Gallina, with the cached configuration
also defined as proof terms in Gallina.
As with \sysname, the \toolnamec extension to \sysnamelong does not extend the TCB in any way (Section~\ref{sec:pi-tcb}).

\subsubsection{The Command}
\label{sec:pi-details-command}

\subsubsection{Differencing}
\label{sec:pi-details-diff}

implementation of the search procedures

the four search procedures so far

how hard it is to define new search procedures

weird details: refolding, historical approach, 

\paragraph{Syntactic Restrictions}
For simplicity, the search procedures currently implemented inside of \toolnamec make some syntactic assumptions
about the input and output types.
For example, the algorithm for differencing algebraic ornaments $\ldots$. 
The details of these restrictions can be found inside of the repository. % TODO link

\subsubsection{Transformation}
\label{sec:pi-details-trans}

There were three hard things about this.
Everything else was standard.

\mysubsubsec{Termination}
When a subterm unifies with a configuration term, this suggests that \toolnamec \textit{can}
transform the subterm, but it does not necessarily mean that it \textit{should}.
In some cases, doing so would result in nontermination.
For example, if \B is a refinement of \Aa, then we can always run \textsc{Equivalence}
over and over again, forever.
%\textsc{Devoid} ruled out this case by simply prohibiting the case where \B refers to \A, but we found it sometimes
%useful to support this case.
We thus include some simple termination checks in our code~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftrules.ml}{\circled{12}}. % liftrules.ml

\mysubsubsec{Intent}
Even when termination is guaranteed, whether to transform a subterm depends on intent.
That is, \toolnamec automates the case of porting \textit{every} \Aa to \B,
but proof engineers sometimes wish to port only \textit{some} $A$s to $B$s.
\toolnamec has some support for this using an interactive workflow~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/minimal_records.v}{\circled{13}},
with plans for automatic support in the future. % minimal_records.v, but show this
%We helped the proof engineer do this by interacting with \toolnamec using a particular workflow.

\mysubsubsec{From CIC$_{\omega}$ to Coq}
The implementation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/lift.ml}{\circled{4}} % lift.ml
of the transformation handles language differences to scale from CIC$_{\omega}$ to Coq.
We use the existing \lstinline{Preprocess}~\cite{Ringer2019} command to turn pattern matching and fixpoints into 
eliminators.
We handle refolding of constants in constructors using \lstinline{DepConstr}.

\subsubsection{Trusted Computing Base}
\label{sec:pi-tcb}

As with \sysname, \toolnamec is implemented as a Coq plugin, and produces terms that Coq type checks in the end.
\toolnamec does not modify the type checker, and furthermore does not add any axioms, so it does not increase the TCB.
This is perhaps even more notable for \toolnamec, since all other implementations of transport across equivalences
that I am aware of at least sometimes add axioms.
Since \toolnamec implements transport as a proof term transformation, it is able to circumvent this and not increase the TCB in any way.

\subsection{Workflow Integration}
\label{sec:pi-workflow}

% PUMPKIN Pi Decompiler & impl
(Segue.)

\subsubsection{Decompiling to Tactics}

\begin{figure*}
\small
\begin{grammar}
<v> $\in$ Vars, <t> $\in$ CIC$_{\omega}$

<p> ::= intro <v> \hspace{0.05cm} | \hspace{0.05cm} rewrite <t> <t> \hspace{0.05cm} | \hspace{0.05cm} symmetry \hspace{0.05cm} | \hspace{0.05cm} apply <t> \hspace{0.05cm} | \hspace{0.05cm} induction <t> <t> \{ <p>, \ldots, <p> \} \hspace{0.05cm} | \hspace{0.05cm} split \{ <p>, <p> \} \hspace{0.05cm} | \hspace{0.05cm} left \hspace{0.05cm} | \hspace{0.05cm} right \hspace{0.05cm} | \hspace{0.05cm} <p> . <p>
\end{grammar}
\vspace{-0.4cm}
\caption{Qtac syntax.}
\vspace{-0.4cm}
\label{fig:ltacsyntax1}
\end{figure*}

\begin{figure*}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Rightarrow$ $p$}\vspace{-0.5cm}\\

\inferrule[Intro]
  { \Gamma,\ n : T \vdash b \Rightarrow p }
  { \Gamma \vdash \lambda (n : T) . b \Rightarrow \mathrm{intro}\ n.\ p }

\inferrule[Symmetry]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathtt{eq\_sym}\ H \Rightarrow \mathrm{symmetry}.\ p }

\inferrule[Split]
  { \Gamma \vdash l \Rightarrow p \\ \Gamma \vdash r \Rightarrow q }
  { \Gamma \vdash \mathrm{Constr}(0,\ \wedge)\ l\ r \Rightarrow \mathrm{split} \{ p, q \}.\ }\\

\inferrule[Left]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(0,\ \vee)\ H \Rightarrow \mathrm{left}.\ p }

\inferrule[Right]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(1,\ \vee)\ H \Rightarrow \mathrm{right}.\ p }

\inferrule[Rewrite]
  { \Gamma \vdash H_1 : x = y \\ \Gamma \vdash H_2 \Rightarrow p }
  { \Gamma \vdash \mathrm{Elim}(H_1,\ P) \{ x,\ H_2,\ y \} \Rightarrow \mathrm{symmetry}.\ \mathrm{rewrite}\ P\ H_1.\ p }\\

\inferrule[Induction]
  { \Gamma \vdash \vec{f} \Rightarrow \vec{p} }
  { \Gamma \vdash \mathrm{Elim}(t,\ P)\ \vec{f} \Rightarrow \mathrm{induction}\ P\ t\ \vec{p} }

\inferrule[Apply]
  { \Gamma \vdash t \Rightarrow p }
  { \Gamma \vdash f t \Rightarrow \mathrm{apply}\ f.\ p }

\inferrule[Base]
  { \\ }
  { \Gamma \vdash t \Rightarrow \mathrm{apply}\ t }
\end{mathpar}
\vspace{-0.4cm}
\caption{Qtac decompiler semantics.}
\label{fig:someantics}
\end{figure*}

\textbf{Transform} produces a proof term,
while the proof engineer typically writes and maintains proof scripts made up of tactics.
We improve usability thanks to the realization that, since Coq's proof term language Gallina is very structured,
we can decompile these Gallina terms to suggested Ltac proof scripts for the proof engineer to maintain.

%\begin{quote}
%\textbf{Insight 3}: The transformed proof terms can then be translated back to tactic scripts.
%\end{quote}

\textbf{Decompile} implements a prototype of this translation~\href{https://github.com/uwplse/coq-plugin-lib/tree/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}: % decompiler.ml
it translates a proof term to a suggested proof script that attempts to prove the same theorem the same way.
Note that this problem is not well defined: while there is always a proof script that 
works (applying the proof term with the \lstinline{apply} tactic), the result is often qualitatively unreadable.
This is the baseline behavior to which the decompiler defaults.
The goal of the decompiler is to improve on that baseline as much as possible,
or else suggest a proof script that is close enough to correct that the proof engineer can
manually massage it into something that works and is maintainable.

\textbf{Decompile} achieves this in two passes: The first pass decompiles proof terms to proof scripts that use a predefined set of tactics.
The second pass improves on suggested tactics by simplifying arguments, substituting tacticals, and using
hints like custom tactics and decision procedures.

\mysubsubsec{First Pass: Basic Proof Scripts}
The first pass takes Coq terms and produces tactics in Ltac, the proof script language for Coq.
Ltac can be confusing to reason about, since Ltac tactics can refer to Gallina terms, and the semantics of Ltac depends both on the
semantics of Gallina and on the implementation of proof search procedures written in OCaml.
To give a sense of how the first pass works without the clutter of these details, we start by defining a mini decompiler that 
implements a simplified version of the first pass.
Section~\ref{sec:second} explains how we scale this to the implementation.

The mini decompiler takes CIC$_{\omega}$ terms and produces tactics in a 
mini version of Ltac which we call Qtac.
The syntax for Qtac is in Figure~\ref{fig:ltacsyntax1}.
Qtac includes hypothesis introduction (\lstinline{intro}),
rewriting (\lstinline{rewrite}), symmetry of equality (\lstinline{symmetry}),
application of a term to prove the goal (\lstinline{apply}), induction (\lstinline{induction}),
case splitting of conjunctions (\lstinline{split}),
constructors of disjunctions (\lstinline{left} and \lstinline{right}), and
composition (\lstinline{.}).
Unlike in Ltac, \lstinline{induction} and \lstinline{rewrite} take a motive explicitly (rather than relying on unification),
and \lstinline{apply} creates a new subgoal for each function argument.
%The implementation reasons about Ltac and so does not make these assumptions.

The semantics for the mini decompiler $\Gamma \vdash t \Rightarrow p$ are in Figure~\ref{fig:someantics} (assuming $=$, \lstinline{eq_sym}, $\wedge$, and $\vee$ are defined as in Coq).
As with the real decompiler, the mini decompiler defaults to the proof script
that applies the entire proof term with \lstinline{apply} (\textsc{Base}).
Otherwise, it improves on that behavior by recursing over the proof term and constructing a proof script using a predefined set of tactics.

\iffalse
\begin{figure*}
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>(@\vspace{-0.04cm}@)
  (@\codesima{list\_rect}@) _ _  (fun (@\codesima{a l H}@) =>(@\vspace{-0.04cm}@)
    (@\codesimc{eq\_ind\_r}@) _ (@\codesimd{eq\_refl}@) (@\codesimc{(app\_nil\_r (rev l) (a::[]))}@))(@\vspace{-0.04cm}@)
    (@\codesime{eq\_refl}@)(@\vspace{-0.04cm}@)
    (@\codesima{y0}@)(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
(@\vspace{-0.14cm}@)
- (@\codesimb{intro y0.}@) (@\codesima{induction y0 as [a l H|].}@)(@\vspace{-0.04cm}@)
  + (@\codesimc{simpl. rewrite app\_nil\_r.}@) (@\codesimd{auto.}@)(@\vspace{-0.04cm}@)
  + (@\codesime{auto.}@)(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
\end{lstlisting}
\end{minipage}
\vspace{-0.3cm}
\caption{Proof term (left) and decompiled proof script (right) for the base case of 
\lstinline{rev_app_distr} (Section~\ref{sec:overview}),  with corresponding terms and tactics 
grouped by color and number.}
\label{fig:rainbow}
\end{figure*}
\fi

For the mini decompiler, this is straightforward: Lambda terms become introduction (\textsc{Intro}).
Applications of \lstinline{eq_sym} become symmetry of equality (\textsc{Symmetry}).
Constructors of conjunction and disjunction map to the respective tactics (\textsc{Split}, \textsc{Left}, and \textsc{Right}).
Applications of equality eliminators compose symmetry (to orient the rewrite direction) with rewrites (\textsc{Rewrite}),
and all other applications of eliminators become induction (\textsc{Induction}).
The remaining applications become apply tactics (\textsc{Apply}).
In all cases, the decompiler recurses, breaking into cases, until only the \textsc{Base}
case holds. % at which point we are done.

While the mini decompiler is very simple, only a few small changes are needed
to move this to Coq.
%The result can already handle some of the example proofs \toolnamec has produced.
The generated proof term of \lstinline{rev_app_distr} from Section~\ref{sec:overview},
for example, consists only of induction, rewriting, simplification, and reflexivity (solved by \lstinline{auto}).
Figure~\ref{fig:rainbow} shows the proof term for the base case of \lstinline{rev_app_distr} 
alongside the proof script that \toolnamec suggests.
This script is fairly low-level and close to the proof term, but it is already something that the proof engineer
can step through to understand, modify, and maintain.
There are few differences from the mini decompiler needed to produce this,
for example handling of rewrites in both directions (\lstinline{eq_ind_r} as opposed to \lstinline{eq_ind}),
simplifying rewrites,
and turning applications of \lstinline{eq_refl} into \lstinline{reflexivity} or \lstinline{auto}.

\mysubsubsec{Second Pass: Better Proof Scripts}
The implementation of \textbf{Decompile} first runs something similar to the mini decompiler, then modifies the suggested tactics to produce a more natural proof script~\href{https://github.com/uwplse/coq-plugin-lib/tree/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}. % decompiler.ml
For example, it cancels out sequences of \lstinline{intros} and \lstinline{revert},
inserts semicolons, and removes extra arguments to \lstinline{apply} and \lstinline{rewrite}. %, ensuring the result still holds. % TODO rewrite
It can also take tactics from the proof engineer (like part of the old proof script) as hints,
then iteratively replace tactics with those hints, checking for correctness.
This makes it possible for suggested scripts to include custom tactics and decision procedures.

\begin{figure}
\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>(@\vspace{-0.04cm}@)
  (@\codesima{list\_rect}@) _ _  (fun (@\codesima{a l H}@) =>(@\vspace{-0.04cm}@)
    (@\codesimc{eq\_ind\_r}@) _ (@\codesimd{eq\_refl}@) (@\codesimc{(app\_nil\_r (rev l) (a::[]))}@))(@\vspace{-0.04cm}@)
    (@\codesime{eq\_refl}@)(@\vspace{-0.04cm}@)
    (@\codesima{y0}@)(@\vspace{-0.04cm}@)
(@\vspace{-0.04cm}@)
- (@\codesimb{intro y0.}@) (@\codesima{induction y0 as [a l H|].}@)(@\vspace{-0.04cm}@)
  + (@\codesimc{simpl. rewrite app\_nil\_r.}@) (@\codesimd{auto.}@)(@\vspace{-0.04cm}@)
  + (@\codesime{auto.}@)(@\vspace{-0.04cm}@)
\end{lstlisting}
\vspace{-0.3cm}
\caption{Proof term (top) and decompiled proof script (bottom) for the base case of 
\lstinline{rev_app_distr} (Section~\ref{sec:overview}), with corresponding terms and tactics 
grouped by color \& number.}
\label{fig:rainbow}
\end{figure}

\mysubsubsec{From Qtac to Ltac}
The mini decompiler assumes more predictable versions of \lstinline{rewrite} and \lstinline{induction}
than those in Coq. \textbf{Decompile} includes additional logic to reason about these tactics~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}. % decompiler.ml
For example, Qtac assumes that there is only one \lstinline{rewrite} direction. Ltac has two rewrite directions,
and so the decompiler infers the direction from the motive.

Qtac also assumes that both tactics take the inductive motive explicitly,
while in Coq, both tactics infer the motive automatically.
Consequentially, Coq sometimes fails to infer the correct motive.
% infers the wrong motive, % without manipulation of goals and hypotheses,
%or fails to infer a motive at all.
%This is especially common for \lstinline{rewrite}, which is purely syntactic.
To handle induction, the decompiler strategically uses \lstinline{revert} to manipulate the goal
so that Coq can better infer the motive.
To handle rewrites, it uses \lstinline{simpl} to refold the goal before rewriting.
Neither of these approaches is guaranteed to work, so the proof engineer may sometimes need to tweak the suggested proof script appropriately.
Even if we pass Coq's induction principle an explicit motive, Coq still sometimes fails due
to unrepresented assumptions.
Long term, using another tactic like \lstinline{change} or \lstinline{refine} before applying these tactics
may help with cases for which Coq cannot infer the correct motive.

\mysubsubsec{From CIC$_{\omega}$ to Coq}
Scaling the decompiler to Coq introduces let bindings, which are generated by 
tactics like \lstinline{rewrite in}, \lstinline{apply in}, and \lstinline{pose}.
\textbf{Decompile} implements~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}} % decompiler.ml
support for \lstinline{rewrite in} and \lstinline{apply in} similarly to how it supports
\lstinline{rewrite} and \lstinline{apply}, except that it ensures that the unmanipulated hypothesis does not occur in the body of the let expression,
it swaps the direction of the rewrite, and it recurses into any generated subgoals.
In all other cases, it uses \lstinline{pose}, a catch-all for let bindings.

\mysubsubsec{Forfeiting Soundness}
While there is a way to always produce a correct proof script,
\textbf{Decompile} deliberately forfeits soundness to suggest more useful tactics.
For example, it may suggest the \lstinline{induction} tactic, but leave the step of motive inference to the proof engineer.
We have found these suggested tactics easier to work with (Section~\ref{sec:search}).
Note that in the case the suggested proof script is not quite correct,
it is still possible to use the generated proof term directly.

\mysubsubsec{Pretty Printing}
After decompiling proof terms, \textbf{Decompile} pretty prints the result~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}.
Like the mini decompiler, \textbf{Decompile} represents its output using a predefined grammar of Ltac tactics,
albeit one that is larger than Qtac, and that also includes tacticals.
It maintains the recursive proof structure for formatting. %, then uses that to print proofs of subgoals using bullet points.
%It displays the resulting proof script to the proof engineer, who can modify it as needed.
%It includes scripts that automate the process of printing all of these tactic proofs to a Coq file,
%in case the proof engineer does not want an interactive workflow.
\toolnamec keeps all output terms from \textbf{Transform} in the Coq environment in case the decompiler does not succeed.
Once the proof engineer has the new proof, she can remove the old one.

\subsubsection{Reaching Real Proof Engineers}

Many of our design decisions in implementing \toolnamec were informed by our partnership with
an industrial proof engineer (Section~\ref{sec:search}).
For example, the proof engineer rarely had the patience to wait more than ten seconds
for \toolnamec to port a term,
so we implemented optional aggressive caching, even caching intermediate subterms
encountered while running the transformation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/cache/caching.ml}{\circled{14}}. % TODO caching.ml
We also added a cache to tell \toolnamec not to $\delta$-reduce certain terms~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/cache/caching.ml}{\circled{14}}.
With these caches, the proof engineer found \toolnamec efficient enough to use on a code base with tens of thousands of lines of code and proof.

 % caching.ml
%These caches are implemented in \href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/src/cache/caching.ml}{caching.ml}.
%or recurse into certain modules.
% set certain terms or modules as opaque to \toolnamec, to prevent unnecessary $\delta$-reduction.

The transformation assumes that all terms are fully \smallmath{$\eta$}-expanded. Sometimes,
however, \smallmath{$\eta$}-expansion is not necessary.
For efficiency, rather than fully \smallmath{$\eta$}-expand ahead of time, \toolnamec \smallmath{$\eta$}-expands lazily, 
only when it is necessary for correctness.

The experiences of proof engineers also inspired new features.
For example, we implemented a search procedure to generate custom eliminators %(\href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/src/automation/search/smartelim.ml}{smartelim.ml})
to help reason about types like $\Sigma$\lstinline{(l : list T).length l = n}
by reasoning separately about the projections~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/search/smartelim.ml}{\circled{15}}. %smartelim.ml
We added informative error messages~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/lib/ornerrors.ml}{\circled{22}} to help the proof engineer distinguish between user errors and bugs. % TODO link to errors
These features helped with workflow integration. % tactic decompiler helped with integration into proof engineering workflows.

(Also whole module processing, somewhere.)



