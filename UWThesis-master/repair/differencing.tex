\section{Differencing}
\label{sec:pumpkin-diff}

% TODO organize into subsections
% TODO either include some algorithm, or include a note about why not

% From Motivating the Core, modified and augmented

Differencing is aware of and guided by the semantics of Coq's rich proof term language \kl{Gallina}---that is what makes it a \kl{semantic differencing} algorithm.
This means that differencing can take advantage of the structure and information carried in every proof term,
thanks to Gallina's rich type theory \kl{CIC$_{\omega}$}.
The rich structure of terms helps guide differencing for each instance of the search procedure,
while the rich information in their types helps ensure correctness in the end.

Consider once again the example from Figure~\ref{fig:example}, but this time not just the base case.
Both versions of the proof are inductive proofs using the same \kl{eliminator}, with slightly different \kl{motives}.
Accordingly, differencing knows that there are two places to look for candidates, namely the base case (line 19)
and the inductive case (lines 20-22).
Differencing breaks each inductive proof into these cases, then recursively calls itself for each case.
In the base case, it finds the candidate from Section~\ref{sec:pumpkin-spec-diff}.
Since this candidate has the candidate goal type (here, the goal type specialized to the base case), differencing knows it has successfully found a candidate.

The rich type information proof terms carry helps prevent exploration of syntactic differences that are not meaningful.
For example, in the inductive case of the proof term from Figure~\ref{fig:example}, the \kl{inductive hypothesis} \lstinline{IHle} (line 21) changes:

\begin{lstlisting}[language=coq]
  $\ldots$ (IHle : (@\diff{n <= m0 + 1}@)) $\ldots$
  $\ldots$ (IHle : (@\diff{n <= m0}@)) $\ldots$
\end{lstlisting}
Notably, though, the type of \lstinline{IHle} changes for \emph{any} two inductive proofs over \lstinline{le}
with different conclusions. A syntactic differencing component 
may identify this change as a candidate.
My semantic differencing algorithms know that they can ignore this change.
This section describes the design of these algorithms.
Section~\ref{sec:pumpkin-impl-diff} describes the implementation in \sysname.

% TODO show examples, formalize if time!!!

Differencing recurses over the structure of two terms $t_a$ and $t_b$ in a common environment $\Gamma$.
When it recurses, it extends $\Gamma$ with common assumptions, then differences subterms.
In each case, it carries a goal type $G$, and returns a list of patch candidates $\vec{t}$ that each have that goal type.
That is, we can view it as a judgment $\Gamma\ \vdash\ (t_a,\ t_b,\ G) \Downarrow_{d} \vec{t}$,
where in the end, for every $t$ in $\vec{t}$, $\Gamma \vdash t : G$.
The details of this vary by subterm-instance combination.

\subsubsection*{Identity}
The simplest patch is the identity patch.
When two terms are \kl{definitionally equal}, % TODO need to explain this somewhere, using latest text from PUMPKIN Pi
differencing infers that the goal is identity,
and returns a singleton list containing only the identity function instantiated to the appropriate type.

\subsubsection*{Application}
When one proof term is a function application, for example:

\begin{lstlisting}
  $\Gamma \vdash (f\ t_a,\ t_b,\ G) \Downarrow_{d} \vec{t}$
\end{lstlisting}
differencing checks to see if $t_b$ is in $f\ t_a$.
That is, it searches for a subterm of $f\ t_a$ that is definitionally equal to $t_b$.
This is how differencing can identify the candidate for the base case of Figure~\ref{fig:example} (line 19).
It is also a core building block that other differencing heuristics rely on.

When both proof terms are function applications:

\begin{lstlisting}
  $\Gamma \vdash (f_a\ t_a,\ f_b\ t_b,\ G) \Downarrow_{d} \vec{t}$
\end{lstlisting}
and the previous heuristic fails,
differencing may recurse into both the functions and the arguments, search for patches, and then compose the results.
How to compose those results varies by instance of the search procedure.

\subsubsection*{Functions} % TODO in Chapter 2, need to mention hypos versus debruijn
The treatment of functions depends on whether a hypothesis or a conclusion has changed.
When recursing into the body of two functions, each with a hypothesis of the same type:

\begin{lstlisting}
  $\Gamma \vdash (\lambda (t_a : T) . b_a,\ \lambda (t_b : T) . b_b,\ G) \Downarrow_{d} \vec{t}$
\end{lstlisting}
differencing assumes that the conclusion has changed.
That is, it assumes that $t_a$ and $t_b$ are the same, adds one of them to a common environment,
and differences the body:

\begin{lstlisting}
  $\Gamma,\ t_a : T \vdash (b_a,\ b_b [t_a / t_b]) \Downarrow_{d} \vec{b}$
\end{lstlisting}
It then filters those candidates $\vec{b}$ to only those with an adjusted goal type $G\ t_a$,
then wraps each candidate $b$ in $\vec{b}$ in a function in the end:

\begin{lstlisting}
  $\lambda (t_a : T) . b$
\end{lstlisting}
with type $G$.

When a hypothesis type has changed:

\begin{lstlisting}
  $\Gamma \vdash (\lambda (t_a : T_a) . b_a,\ \lambda (t_b : T_b) . b_b,\ G) \Downarrow_{d} \vec{t}$
\end{lstlisting}
differencing acts similarly, but it substitutes the changed hypothesis type in the body in order to recurse into a well-typed environment.
It also has some additional logic to remove hypothesis that need not show up in the goal type.

\subsubsection*{Eliminators} % From implementation, plus some additional stuff
Recall that \kl{inductive types} in CIC$_{\omega}$ come equipped with \kl{primitive eliminators}.
The semantic differencing algorithms views inductive types as \emph{trees} that represent these eliminators.
In these trees, every node is a type context, and every edge is an extension to that type context 
with a new term. %\footnote{These trees are inspired by categorical models of dependent type theory~\cite{Hofmann97}.}
Correspondingly, type differencing (to identify goal types) compares nodes, 
and term differencing (to find candidates) compares edges. 

\begin{figure}
\begin{center}
\includegraphics[scale=0.55]{repair/nat_ind}
\end{center}
\caption{The type of (left) and tree for (right) the eliminator of \lstinline{nat}. The solid edges represent hypotheses, and the dotted edges represent the proof obligations for each case in an inductive proof.} % TODO use Pi instead of forall
\label{fig:cattree}
\end{figure}

The key benefit to this model is that it provides a natural way to express inductive proofs, so
that differencing can efficiently identify candidates.
Consider, for example, searching for a patch between conclusions of two inductive proofs of theorems about the natural numbers:

\begin{lstlisting}
  $\mathrm{Elim} (\mathrm{nat},\ P)\ \{ f_0,\ f_S \}$
  $\mathrm{Elim} (\mathrm{nat},\ Q)\ \{ g_0,\ g_S \}$ 
\end{lstlisting}
with goal type:

\begin{lstlisting}
  $Q \rightarrow P$
\end{lstlisting}

Differencing looks in both the base case and in the inductive case for candidates.
In each case, differencing diffs the terms in the dotted edges of the tree for the eliminator of \lstinline{nat} (Figure~\ref{fig:cattree}) to
try to find a term that maps between conclusions of that case:

\begin{lstlisting}[language=coq]
  $\Gamma \vdash (f_0,\ g_0,\ Q\ 0 \rightarrow P\ 0) \Downarrow_{d} \vec{t_0}$
  $\Gamma \vdash (f_S,\ g_S,\ \Pi (n : \mathrm{nat}) . Q\ (S\ n) \rightarrow P\ (S\ n)) \Downarrow_{d} \vec{t_S}$
\end{lstlisting}
where \lstinline{O} is shorthand for \lstinline{Constr (0, nat)}, and \lstinline{S} is shorthand for \lstinline{Constr (1, nat)}.
In the inductive case, differencing also knows that the change in the type of the \kl{inductive hypothesis} is not semantically relevant (it occurs for any change in the inductive \kl{motive}).
Furthermore, it knows that the inductive hypothesis cannot show up in the patch itself, since the goal type does not reference the inductive hypothesis,
so it attempts to remove any occurrences of the inductive hypothesis in any candidate.

When differencing finds a candidate, it knows $Q$ and $P$ as well as the arguments $0$ or $S\ n$.
This makes it simple for \sysname to later query the transformations for the final patch, with type $Q \rightarrow P$.


%One idea for this is to draw on work in change and dependency management~\cite{873647, Autexier:2010:CMH:1986659.1986663, Celik:2017:IRP:3155562.3155588} to identify changes,
%then use the factoring transformation to break those changes into smaller parts.


