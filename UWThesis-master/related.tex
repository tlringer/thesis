\chapter{Related Work}
\label{sec:related}

% TODO whatever else isn't here yet, and some of this might be factored out or partially factored out---all papers, including survey, plus generals

TODO somewhere here or elsewhere (if elsewhere, fix references): talk about what lessons carry over to automated theorem provers,
and which lessons carry over to other ITPs, and what work is needed to reach those tools.

\section{Programs}

\subsection*{Program Repair} 

% From sysname Pi, unchanged

Proof repair can be viewed as a form of \textit{program repair}~\cite{Monperrus:2018:ASR:3177787.3105906, Gazzola:2018:ASR:3180155.3182526}
for proof assistants.
Proof assistants like Coq are a good fit for program repair: A recent paper~\cite{Qi:2015:APP:2771783.2771791} 
recommends that program repair tools draw on extra information
such as specifications or example patches. In Coq, specifications and examples 
are rich and widely available: specifications thanks to dependent types,
and examples thanks to constructivism.

% From sysname PATCH, unchanged

Program repair tools for 
languages with non-dependent type 
systems~\cite{Pei:2014:APR:2731750.2731779, Long:2016:APG:2837614.2837617, Le:2017:SSS:3106237.3106309, Mechtaev:2016:ASM:2884781.2884807, Monperrus2015} 
may have applications in the context of a dependently typed language.
Similarly, our work may have applications within program repair in these languages:
Future applications of our approach may repurpose it to repair programs for functional languages.

% From Generals, unchanged

While proof repair tools like \sysname\ are new, the broader domain of program repair is well-established.
A recent survey paper in program repair~\cite{Monperrus:2018:ASR:3177787.3105906} distinguishes between different repair tools
based on the \textit{oracle} they use to determine what behavior is correct and thus what it means to fix the behavior. 
Examples of oracles include test suites (\textit{test-based} repair) and specifications (\textit{specification-based} repair);
proof repair is a kind of specification-based repair.
%outputs of static analyses, and invariants. 
%Lessons and techniques from program repair tools provide useful insights for proof repair. % TODO uninspiring intro
This overview starts with a discussion of lessons learned from recent results in test-based program repair that have revealed
problems with existing tools and research methodologies (Section~\ref{sec:lessons}). It then discusses
techniques from existing program repair tools that are relevant to proof repair (Section~\ref{sec:techniques}). 
In the language of the program repair survey, %~\cite{Monperrus:2018:ASR:3177787.3105906},
this overview focuses on behavioral repair, or patching the code, as opposed to state repair, or patching the dynamic behavior;
more information on other repair tools %and on behavioral repair tools not immediately relevant to the \sysname\ vision
can be found in the survey.

\subsubsection{Lessons from Test-Based Program Repair}
\label{sec:lessons}

A recent paper~\cite{Qi:2015:APP:2771783.2771791} calls results from test-based program repair into
question. The paper shows that most of the reported patches generated by the popular test-based program repair tools
are not correct, and that a tool which only removes functionality
performs as well %on the evaluation suites 
as these tools. These observations %from this paper
are later reaffirmed in a different setting~\cite{DBLP:journals/corr/abs-1811-02429}.
A close look shows that these observations highlight why proof repair
is a promising flavor of program repair, and carry important lessons
for proof repair.

\paragraph{Tools Evaluated}
The paper calls into question
the patches produced by three test-based program repair tools:
GenProg~\cite{LeGoues:2012:SSA:2337223.2337225}, RSRepair~\cite{Qi:2014:SRS:2568225.2568254}, and AE~\cite{Weimer:2013:LPE:3107656.3107702}.
%Initial results suggested that these tools are extremely effective; the review calls these results into question.
%We briefly explain each of these tools for context.

GenProg uses genetic programming to identify patches: It discovers variants of repaired programs, then uses a fitness function derived from test cases
to evaluate those variants. It forms its search space for patches based on existing code elsewhere in the program,
then inserts, deletes, and swaps code and control flow to produce variants. It evaluates whether a patch is
a success by ensuring that tests in the test suite that ought to pass do pass, and tests in the test suite that ought to fail do fail.
It also attempts to minimize the difference between the original and fixed program.
GenProg has been used as the baseline for comparison for many other tools, and has over 120 citations.

RSRepair uses random search, a simpler technique that does not use a fitness function. % TODO 139, are there other papers to cite here? Is the one cited 139?
RSRepair also attempts to limit the search space.
The authors recommend using random search as a baseline for comparison for other tools;
this has been done many times, and the paper has over 140 citations.

AE utilizes both a program equivalence relation and % TODO read in more detail and make sure you understand
adaptive search, a technique which iteratively adapts solutions to simpler search problems.
AE first identifies patch candidates using a program equivalence relation, then uses a sequence of 
two adaptive search algorithms to identify a patch from those candidates.
It minimizes the cost of the patch according to a cost model.
AE has also been used as a baseline for many comparisons, and has over 130 citations.

\paragraph{Why These Tools Produce Incorrect Patches}
The paper notes several reasons for the discrepancy 
between reported and actual patch correctness results. For one, tests in the evaluation suites for these tools
are underspecified; for example, some tests check whether
a program terminates with an exit code of 0, but do not check the program output.
In addition, patches are often overfit to the tests in the test suite; additional tests expose problems with those patches.
In fact, some patches are outright harmful, as they introduce new problems which the test suite
does not check for.

This discrepancy is not solely a property of the test suites in question;
running the same tools on strengthened versions of the test suites produces no patches at all.
The authors postulate that this may be because the correct patches for the stronger test suites are not in the
search space for these tools, or because they are in the search space, but the tools
cannot find them efficiently enough. 

\paragraph{Producing Correct Patches}
To improve program repair tools in the future, the paper recommends richer search 
spaces and search algorithms that can effectively search those enriched search spaces. 
In addition, the paper recommends augmenting program repair techniques with additional information outside of
tests, such as code from other applications, example patches, or specifications.

Many test-based program repair tools meet these criteria, some developed before these results came to light,
and some in response to these results. 
SemFix~\cite{nguyen2013semfix}, for example,
generalizes test suites to specifications, which the authors claim improves the performance of search.
%It uses symbolic execution to generate specifications that encode passing the test suite, then synthesizes patches that satisfy the specification. 
Similarly, NOPOL~\cite{Xuan:2017:NAR:3071893.3071964} repairs a certain class of bugs by generating and solving SMT formulas from a test suite.
Other tools that augment test-based techniques with constraint problems include
directFix~\cite{Mechtaev:2015:DLS:2818754.2818811} and SearchRepair~\cite{Ke:2015:RPS:2916135.2916260}. % TODO are there more? If so, add

Some tools avoid using test suites to judge correctness of candidate patches, and instead
bring the programmer into the loop. For example, both ReAssert~\cite{daniel2009reassert}
and QACrashFix~\cite{gao2015fixing} suggest repairs directly to the programmer. % TODO is this true? accurate?
% TODO more here if they exist
Opad~\cite{Yang:2017:BTC:3106237.3106274} provides an alternative to expecting the test-based repair tool
to make use of extra information. Instead, Opad automatically detects overfitted patches and improves existing test suites.
The evaluation shows that using Opad to improve test suites improves the performance of several test-based program repair tools. % TODO accuracy check

Prophet~\cite{Long:2016:APG:2837614.2837617} and Relifix~\cite{Tan:2015:RAR:2818754.2818813} use example patches or code that used to be 
correct in order to identify or rank patches, and SPR~\cite{Long:2015:SPR:2786805.2786811} uses a novel technique to effectively search a rich search space;
more information on tools that use techniques especially relevant to \sysname\ can be found in Section~\ref{sec:techniques}.

It is not clear to what extent the claims many of these tools make would stand up to the evaluation methodology from this paper.
The NOPOL paper inadvertedly highlights  when it uses the same example as the SemFix paper,
but finds a different patch which is also correct, since the tests are underconstrained; it's unclear
if both of these patches actually \textit{behave} correctly. % TODO check if the NOPOL paper addresses this
Reevaluating these tools with the strengthened test suites may help
confirm whether the recommendations for improving test-based repair tools have already been realized
effectively. % TODO does this exist anywhere?

\paragraph{Implications for Proof Repair}
These observations show why proof repair is particularly promising. In the world of proof repair, there is always additional information,
since the theorem that the tool patches is a specification.
This means that as long as the specification is correct, these tools cannot produce
incorrect patches. \sysname\ goes one step further and uses more additional information
by deriving patches from example patches, which makes search more efficient.

These observations also carry important lessons for proof repair.
For one, many of the other techniques that the authors recommend for augmenting repair tools with
additional information may be useful in the context of proof repair.
It may be particularly interesting to look at techniques that have already been used
successfully in other tools. For example, SemFix uses a synthesis technique;
the analogue to this in the context of proof repair is proof search. 
While the underlying logics are different, if a proof repair tool
can state its problem as a classic proof search problem, then higher-level ideas
from these program repair tools may cross over.

% TODO more in paragraph about implications of some of the tools that do correct

In addition, proof repair tools should be careful not to trim the search space so that it no
longer contains correct patches.
Furthermore, just as tests can test useless properties, so theorems can state useless properties;
in order for proof repair tools to be useful, the specification must actually be correct.
Any proof repair tool that changes the specification itself should come equipped with a proof (be it 
internal or metatheoretical) that the specifications it produces are meaningful.

In evaluating proof repair tools, it is important to compare to an acceptable baseline; 
manual inspection of patches~\cite{DBLP:journals/corr/abs-1811-02429} 
may provide reassurance that patches are correct.

\subsubsection{Program Repair Techniques}
\label{sec:techniques}
% TODO maybe break into subsubsections without high-level technique sections, depends how long they are
% TODO or better: split into techniques that sysname already uses in a sense, then another subsubsection for techniques
% future sysname might want to use or learn from

%\sysname\ is, in a sense, a program repair tool for Coq. 
%In the language of the survey on program repair~\cite{Monperrus:2018:ASR:3177787.3105906},
\sysname\ is specification-based, since it uses a specification (a goal type derived from the difference
between the old and new type) as the oracle to judge whether a patch (a term) is correct (has that type). To identify
patches, \sysname\ uses program differencing to identify candidate patches from example changes, 
then generalizes those candidates into patches that fix broken proofs that used to go through; existing program repair tools also generalize
examples and repair regressed code.
Tools that these techniques can offer lessons for how to improve those techniques within proof repair tools like \sysname.
%and tools that use different techniques may provide leads on how to integrate new techniques into \sysname\ in the future.
%This overview samples a number of program repair tools and techniques that are relevant to \sysname,
%then discusses the implications for \sysname.

\paragraph{Regression Repair} Regression repair tools target regression bugs,
or bugs introduced by changes in code.
In a sense, \sysname\ is a regression repair tool for Coq, as it repairs regression bugs in proofs:
proofs that used to succeed, but because of some change in specification, no longer succeed.

Test-based regression repair tools repair code that causes a set of tests (the \textit{regressed tests}) that used to pass to no longer pass,
such that regressed tests pass on the repaired code.
One such tool is ReAssert~\cite{daniel2009reassert} for Java,
which focuses on regressions caused by refactoring.
ReAssert uses a program analysis to identify broken code,
chooses a strategy for repair, and suggests repairs to the programmer using that strategy that cause regressed tests to pass.
It loops through strategies until one works or none remain.
% TODO results?
Another such tool is Relifix~\cite{Tan:2015:RAR:2818754.2818813} for C.
%which does not limit itself to refactorings.
Relifix uses a manual inspection to find code transformations based on regressions, then searches those transformations for 
patches that make the regressed tests pass without making other tests fail. % TODO read in more detail, add info if necessary
% TODO results?

% fix up transition or whatever
While not a regression repair tool, GRAFTER~\cite{Zhang:2017:ATD:3097368.3097448} is a related tool that adapts the tests themselves,
rather than the code under test. Its focus is on testing software clones for errors introduced during the
cloning process. It uses a static analysis to identify variables and methods that correspond between the clones,
then ensures that the flow is preserved using that mapping. The user can then run the new tests to compare behavior.
It provides a guarantee about type safety, and it performs reasonably well on some real-world software.
% accuracy check and clean up and so on

\paragraph{Repair by Example} % TODO can we really call them this? Maybe don't categorize like this, just mention technique
Some program repair tools generalize example changes into patches that repair the broken program.
\sysname\ is one such tool, since it generalizes example changes in proofs that address some breaking change
into a patch that can fix other broken proofs. 

Prophet~\cite{Long:2016:APG:2837614.2837617}, a test-based
repair tool for C,
uses human-generated patches from software repositories as examples.
These examples can come from different applications from the one that is being repaired.
Prophet uses differencing over ASTs % of the unpatched and patched code 
to extract \textit{features} that describe the behavior of the example patch abstracted from its particular application. % TODO is this correct, roughly?
From these patches, it learns a model of correct code. Then,
it localizes faults and generates candidates, which it ranks according to the learned model.
In this way, it produces patches that not only cause the tests to succeed, but also
are likely according to the learned model to be correct to humans. % TODO how are the results?

% TODO also note that they change the Oracle to the human instead of tests because of controversy
The repair tool QACrashFix~\cite{gao2015fixing} uses pairs of buggy and fixed code from Q \& A sites like
StackOverflow to derive patches for crashing input bugs. These patches are in the form of edit scripts,
so that they can apply in different contexts. %The idea here is that many bugs recur, so they look for fixes that apply to fix other bugs. 
It uses a preprocessing step to find the right query for the Q \& A site, then they
look at answers for buggy and fixed code examples, then from those they derive edit scripts to try to fix the bug.
It then uses a combination of tests and human validation to determine whether the patches are correct. % TODO is this accurate?
% TODO results?
% TODO fix ups

SearchRepair~\cite{Ke:2015:RPS:2916135.2916260} turns code from repositories into a searchable database.
To form this database, it uses a static analysis to encode the input-output behavior of the code as constraints for an SMT solver.
It then localizes the fault in the buggy program,
encodes the buggy program similarly, performs a semantic code search over that database to identify candidate patches,
and finally uses the test suite as an oracle to determine whether candidates succeed. % TODO anything else worth mentioning?
% TODO results?

Systematic editing~\cite{meng2011systematic} is a technique that could help repair by example tools.
This technique generalizes an edit to a program into a program transformation that can apply in similar program contexts.
It works by syntactic differencing over the AST of the example edit, abstracting the difference, and applying it elsewhere.
It can handle insertions, deletions, updates, and moves.
\textsc{Lase}~\cite{meng2013lase} implements and improves on this,
making use of multiple examples instead of just one, % maybe we should too? for implications for sysname section
and also automatically identifying locations to apply transformations.
Similarly, \lstinline{spdiff}~\cite{andersen2010generic} generalizes patches into semantic patches for Cocinelle~\cite{padioleau2008documenting},
which can then apply those patches automatically in different contexts. This way, the library designer can write a semantic patch
himself, or \lstinline{spdiff} can infer one.
% TODO language? results? guarantees? limitations? for each of these

A natural integration point for a tool like this is at the IDE level. % does this belong here? or whatever?
CatchUp!~\cite{Henkel:2005:CCR:1062455.1062512} is an IDE plugin (implemented for Eclipse in Java) that automatically adapts library clients to API refactorings.
It records refactorings that the library developer makes inside of the IDE,
then replays the refactorings in in client code, reconstructing everything from the recorded trace. % TODO accuracy check
% TODO results? guarantees? limitations?

% TODO maybe, if time: sun2008automated, sun2010propagating

%\paragraph{Staged Program Repair} % TODO better intro here I guess
%\textit{Staged program repair}~\cite{Long:2015:SPR:2786805.2786811} is a technique to search a rich search space effectively.
%This technique involves using staged repair operators, and discarding candidates that cannot be patches as the tool iterates. % TODO accurate? reread
%It has some similarities to the way that \sysname\ uses intermediate goals to identify candidate patches before it generalizes them.
% TODO really sysname has some similarities to it; also fix up this paragraph it sucks

%%The tool that implements this technique is SPR. % TODO what language?
%It makes use of a number of repair strategies which can be parameterized, so that instead of each strategy
%representing a single program transformation, it instead represents a class of transformations.
%It then tries to find a parameter with which it can instantiate that strategy. If it can't
%instantiate the strategy, it moves on other strategies.
%In this way, % TODO how does this connect?
%it can rule out many candidate patches ahead of time, which makes the search space for a correct patch much smaller.
% TODO results, and how this connects to Prophet
% TODO only put back in if we actually need it

% TODO read and write about

%\paragraph{Static Analysis for Program Repair}
%ReAssert~\cite{daniel2009reassert} again?

% TODO do this only if time

%\paragraph{Logozzo and Ball}

%Static analysis

% TODO 104

%\paragraph{Gupta et al.}

%Static analysis, repairing compiler errors, uses language model based on deep learning

% TODO 59

\paragraph{Specification-Based Repair} Some tools use specifications as an oracle, like \sysname.
Sometimes these tools combine this with other oracles like tests. % TODO fix up a lot
For example, Automated Debugging Using Path-Based Weakest Preconditions~\cite{10.1007/978-3-540-24721-0_20} % TODO does this have a reasonable name of a tool?
uses pre and post conditions written in FOL to locate buggy statements and generate repairs, 
then runs tests to verify the repairs. 
%are successful. % TODO check for accuracy
% TODO does it use solvers or what? seems to do some kind of differencing, then modification based on that, but really confusing, need to read in detail
AutoFix-E~\cite{Wei:2010:AFP:1831708.1831716, pei2014automated} uses contracts
to repair Eiffel programs. % TODO also something about repair templates; read in more detail maybe
Specification-Based Program Repair Using SAT~\cite{gopinath2011specification} % TODO ugh name your tools guys
encodes pre and post conditions in combination with other constraints from the code % TODO accuracy check
into SAT and then uses Alloy to generate patches. % TODO accuracy check again

%TODO:  This paper~\cite{konighofer2011automated}
%Pre and post conditions, assertions, SMT
% TODO read and add if it belongs

% TODO cite Harper [8] from the paper, which the methodology is apparently based on
%\sysname\ uses theorem statements as specifications. In the world of program repair,
%there is one methodology that does something similar, though it does not repair proofs directly.
%Instead, 
Proof-directed repair~\cite{dennis2006proof} presents a methodology
for repairing programs based on information from incomplete proofs in Isabelle.
Essentially, the programmer writes a proof, and then uses feedback
from the attempted proof to debug and fix the code.
In a sense, since if the repair succeeds the proof should go through,
it uses proofs as an oracle. The paper presents a few techniques for fixing the broken code,
then shows some examples using those techniques with existing tools.
It does not yet automate it in a tool.

%\paragraph{Reflexive Parsing}

% TODO A Reflexive and Automated Approach to Syntactic Pattern Matching in Code Transformations -- only discuss
% if more relevant and if time
% TODO what paper is this? find citation

%\begin{enumerate}
%\item What they did:
%\begin{enumerate}
%\item All about helping users write code transformations for tool-assisted refactoring without making them learn details of the IR of the language they're %writing code transformations for%
%\item From the user's perspective, just a syntactic pattern that extends the base language
%\item Inference engine goes from syntactic pattern to IR (this was already done before their paper though)
%\item But language-independent implementations of this are hard, so they address a bunch of challenges for this
%\end{enumerate}
%\item How they did it:
%\begin{enumerate}
%\item Using a parser generator to address those challenges
%\item Parser generator generators a parser, which acts as the inference engine to generate the IR
%\item So you can add just a single line to the grammar of the language to turn on syntactic pattern matching
%\end{enumerate}
%\end{enumerate}

\paragraph{Implications for Proof Repair}

\sysname\ uses similar techniques to existing tools, though it is in a different language
and so encounters different challenges. Some things are in fact easier for \sysname\ than for other tools;
the richness of specifications tells \sysname\ exactly when a patch is successful, for example. 
But the proof assistant \sysname\ operates over supports dependent types, and in the world of proof repair,
the criteria for a patch being correct is very strict. 

Even with these differences, proof repair tools like \sysname\ can learn a lot from other tools that use these techniques.
Consider regression repair tools. %While test-based techniques like those used by ReAssert and Relifix
%don't transfer over in a straightforward way, \sysname\ can learn something from both tools.
Both ReAssert and Relifix support something similar to a kind of change in the \sysname\ sense:
strategies for ReAssert, and operators for Relifix. Relifix uses a manual insection of changes made to determine which operators to support;
when extending \sysname, using the methodology from this inspection may help identify initial kinds of changes for \sysname to support, even though
in the long term, automating this process is ideal (which the Relifix paper similarly notes).
ReAssert also suggests repairs to programmers rather than apply them directly; % TODO check accuracy
supporting this mode of interaction in \sysname\ would integrate
naturally with the typical ITP workflow.

GRAFTER also contributes lessons for proof repair tools like \sysname.
For one, it is essentially a differencing tool: It identifies significant differences between clones and uses those differences
to reuse tests, while ignoring changes that are expected. Interestingly, GRAFTER adapts the tests themselves, rather than adapting code to pass
those tests; this is akin to how in the world of proof repair, sometimes it makes sense not only to change the proof,
but also to change the theorem statement itself. Understanding both the differencing technique and how to adapt the oracle for correctness
along those differences could certainly help future developments.
Still, GRAFTER uses a much simpler type system than that supported by most ITPs like Coq, and restricts the kinds of changes allowed relative to what
is desirable in a proof repair tool like \sysname. % TODO implications of simpler type system and so on

Several of the repair by example tools make use of code repositories and libraries to identify examples.
The long-term vision for \sysname\ discusses using both of these sources, but so far there is little automation
to do so. Making use of code repositories is especially difficult, since Github commits are typically large.
While the research plan mostly considers how to work around this, % by looking at the IDE level instead,
these tools may offer some insights for how to approach this problem.

Many of these tools can also make use of code from different contexts, for example from different libraries. % altogether.
For this, Prophet uses universal features, QACrashFix generalizes patches identified by a differencing algorithm in one context to another context,
SearchRepair uses indexes code by its input-output behavior as constraints for an SMT query,
and \textsc{Lase} makes use of systematic editing to adapt program transformations to different contexts.
Ideally, a tool like \sysname\ could also handle code from different contexts. % what makes it ``different''? do I mean ``similar''? what is the ``same'' context here?
Combining these ideas with work on hammers or proof reuse may help solve this problem
in an ITP context.

Several repair by example tools can also make use of multiple examples, as opposed to just one.
While in the proof repair world, one example is often % is it? why?
enough, perhaps existing techniquess have something to contribute with respect to handling multiple examples. % like machine learning or whatever; but honestly, I doubt it

CatchUp! also provides a useful model for IDE integration for a proof repair tool like \sysname.
For example, it might be useful to record changes within a project inside of an IDE 
so that \sysname\ can later on use those changes to find patches in other contexts. % clarify this nonsensical explanation
Additionally, the \sysname\ paper identifies a need for library designers providing patches for clients,
so that clients can circumvent the search functionality; something like the trace file CatchUp! 
uses may provide useful insight on how to accomplish this. % do we need this? sounds weird in our context
The use of comments at the library developer level is also potentially useful, as is the idea of a simple log for manual replay.

%\sysname, like SPR, discards candidates that can't pass as it goes
%Staged program repair: SPR is similar to \sysname\ in the way it discards candidates that can't pass as it goes, the way we do intermediate type goals.
%Repair strategies that can be parameterized that represent classes of transformations are kind of like strategies in \sysname\ or whatever we call them.
%The way it rules out a lot of candidates as it goes instead of generating a bunch of repairs, trying those, failing, and revisiting the search is also similar
%to \sysname\ and its intermediate type goals that need to be satisfied.
%This general technique probably could actually be useful for Coq. The idea of parameterizing classes of transformations could be useful,
%especially when we have a lot of different kinds of changes and maybe aren't even sure which one to apply (search might be hard to scale for this)
%or have multiple changes in succession.
%Maybe there are intermediate unification problems that are not difficult that could rule out a lot of candidates.
%Obviously, we don't need tests since we have specifications.
%I guess the interesting part would be whether there are ways to parameterize transformation classes similarly to SPR that make the search or unification problem
%tractable, and make it more efficient than just naively scaling the way we currently do things. I honestly don't have an answer.
%The kinds of repairs they're concerned with aren't as much of a thing in proof repair.

% TODO only include SPR stuff if it feels like it's missing
% TODO only if time: Static analysis: stuff

Specification-based repair tools use
the same kind of oracle \sysname\ uses,
though there is a lot of diversity
within these tools. %For example, several tools use
%pre and post conditions, which are not as relevant
%for \sysname, which makes use of proofs.
Some of these tools such as AutoFix-E make
use of existing automation in the form of constraint
solvers to identify patches that satisfy specifications;
making use of Coq's existing automation
in this way might prove fruitful for a tool like \sysname.
The most similar technique is the proof-directed repair methodology. 
\sysname\ focuses
only on repairing proofs, and not
on repairing functions; perhaps using partial 
proofs as in proof-directed repair
can help a tool like \sysname\
repair functions as well.

%Reflexive parsing:
%\begin{enumerate}
%\item How it is useful to us:
%\begin{enumerate}
%\item Could think about writing patches for refactorings by hand
%\item Could be useful when we think about some changes as code transformations, especially for refactorings (mentioned in sysname paper)
%\item Then we could also think about searching for them, not just writing them
%\item Also could help when thinking not only about Coq, but other proof assistants, when we're thinking about some language-independence
%\end{enumerate}
%Where it falls short for us:
%\begin{enumerate}
%\item Coq already has its own evar system so why not just reuse that? Or its notation system somehow? Or see how those are implemented?
%\item Only refactorings, which are a small piece
%\item No idea how Coq's parser works but it's weird
%\end{enumerate}
%\end{enumerate}

% TODO any other stuff

% ---

%\subsubsection{Orphaned}

%TODO move these elsewhere as needed

%\paragraph{Weimer}

%Abstract behavioral model, patch generation technique, safety policy, static

% TODO 187

%\paragraph{Pachika}

%Abstract behavioral model, Java

% TODO 30

%\paragraph{Modular and Verified Automatic Program Repair}

% TODO 104

% TODO more at some point

\subsection*{Ornaments}

% From DEVOID, unchanged

\toolnameb automates discovery of and lifting across algebraic ornaments in a higher-order dependently typed language.
In the decade since the discovery of ornaments~\cite{mcbride}, there have been a number
of formalizations and embedded implementations of ornaments~\cite{Dagand:2013:CTO:2591370.2591396, ko2013relational, dagand2014transporting, ko2016programming, dagand2017essence}.
\toolnameb is the first tool for ornamentation to operate over a non-embedded dependently typed language.
It essentially moves the automation-heavy approach of Ornamentation in ML~\cite{Williams2017},
which operates on non-embedded ML code, into the type theory that forms the basis of theorem provers like Coq. 
In doing so, it takes advantage of the properties of algebraic ornaments~\cite{mcbride}.
It also introduces the first search algorithm to identify ornaments, which in the past 
was identified as a ``gap'' in the literature~\cite{ko2016programming}.

\subsection*{Programming by Example}

% From sysname PATCH, unchanged

Our approach generalizes an example that the programmer provides.
This is similar to programming by example, a subfield of 
program synthesis~\cite{DBLP:journals/ftpl/GulwaniPS17}. 
This field addresses different challenges in different logics,
but may drive solutions to similar problems in a dependently typed language.

\subsection*{Differencing \& Incremental Computation}

% From sysname PATCH, unchanged

Existing work in differencing and incremental computation may help 
improve our semantic differencing component.
Type-directed diffing~\cite{Miraldo:2017:TDS:3122975.3122976}
finds differences in algebraic data types.
Semantics-based change impact analysis~\cite{Autexier:2010:SCI:1860559.1860580} models semantic differences
between documents.
Differential assertion checking~\cite{differential-assertion-checking-2} analyzes different
versions of a program for relative correctness with respect to a specification.
Incremental $\lambda$-calculus~\cite{Cai:2014:TCH:2594291.2594304} introduces a general model for program changes.
All of these may be useful for improving semantic differencing.

\section{Proofs}

\subsection*{Proof Reuse}

% From sysname PATCH, mostly unchanged

Our approach reimagines the problem of proof reuse in the context of proof automation.
While we focus on changes that occur over time, traditional proof reuse techniques can help
improve our approach.

Proof reuse for extended inductive types~\cite{Boite2004} adapts proof obligations
to structural changes in inductive types. Later work~\cite{Mulhern06proofweaving} proposes a method
to generate proofs for new constructors. These approaches may be useful when extending the differencing
component to handle structural changes. Existing work in theorem reuse and proof generalization~\cite{Felty1994, pons00, Johnsen2004} abstracts existing proofs for reusability, and may be useful
for improving the abstraction component. % TODO much more generalization work should be cited here; see stuff from UIUC interview follow-up email, maybe move into sysname PATCH chapter
Our work focuses on the components critical to searching for patches; these complementary approaches
can drive improvements to the components.

% From sysname Pi, unchanged

A few proof reuse tools work by proof term transformation and so can be used for repair.
Existing work~\cite{Johnsen2004} describes a transformation that generalizes theorems in Isabelle/HOL.
\toolnamec generalizes the transformation from \textsc{Devoid}~\cite{Ringer2019},
which transformed proofs along algebraic ornaments~\cite{mcbride}.
Magaud \& Bertot 2000~\cite{magaud2000changing} implement a proof term transformation between
unary and binary numbers. 
Both of these fit into \toolnamec configurations,
and none suggests tactics in Coq like \toolnamec does.
The expansion algorithm from Magaud \& Bertot 2000~\cite{magaud2000changing} may help guide the design
of unification heuristics in \toolnamec.

% From sysname PATCH, unchanged

Existing work in proof reuse focuses on transferring proofs between isomorphisms,
either through extending the type system~\cite{Barthe:2001:TIP:646793.704711} or through an automatic method~\cite{Magaud2002}.
This is later generalized and implemented in Isabelle~\cite{Huffman2013} and Coq~\cite{ZimmermannH15, tabareau:hal-01559073};
later methods can also handle implications.

% From sysname Pi, unchanged

The widely used Transfer~\cite{Huffman2013} package supports proof reuse in Isabelle/HOL. % TODO somewhere discuss other proof assistants
Transfer works by combining a set of extensible transfer rules with a type inference algorithm.
Transfer is not yet suitable for repair, as it necessitates maintaining references to both datatypes.
%In addition, the proof assistant Isabelle/HOL that Transfer works for lacks both dependent types and proof terms.
One possible path toward implementing proof repair in Isabelle/HOL may be to reify proof terms using something like
Isabelle/HOL-Proofs, apply a transformation based on Transfer, and then (as in \toolname) decompile those terms to automation that does not apply Transfer or refer to the old datatype in any way.

CoqEAL~\cite{cohen:hal-01113453} transforms functions across relations in Coq,
and these relations can be more general than \toolnamec's equivalences.
However, while \toolnamec supports both functions and proofs, CoqEAL supports only simple functions
due to the problem that \lstinline{Iota} addresses.
CoqEAL may be most useful to chain with \toolnamec to get faster functions.
Both CoqEAL and recent ornaments work~\cite{williamsphd} may help with
better workflow support for changes that do not correspond to equivalences.

The \toolnamec transformation implements transport.
Transport is realizable as a function given univalence~\cite{univalent2013homotopy}.
UP~\cite{tabareau2017equivalences} approximates it
in Coq, only sometimes relying on functional extensionality.
While powerful, neither approach removes references to the old type. %making them poorly suited for repair.

Recent work~\cite{tabareau2019marriage} extends UP with 
a white-box transformation that may work for repair.
This imposes proof obligations on the proof engineer beyond those imposed by \toolname,
%that establish what is effectively the correctness criteria
%for the configuration in \toolname, while \toolname needs only that it holds metatheoretically.
and it includes neither search procedures for equivalences nor tactic script generation.
It also does not support changes in inductive structure,
instead relying on its original black-box functionality;
\lstinline{Iota} solves this in \toolname. % and is based on lessons learned from reading that article.
The most fruitful progress may come from combining these tools. % to take advantage of the benefits of both.

% From DEVOID, mostly unchanged

\toolnameb identifies and lifts proofs along a specific equivalence 
similar to that from existing ornaments work~\cite{ko2016programming}.
The need to automatically lift functions and proofs
across equivalences and other relations is a long-standing challenge for proof 
engineers~\cite{magaud2000changing, barthe2001type, magaud2003changing, huffman2013lifting, zimmermann2015automatic, cohen:hal-01414881}.
The univalence axiom from Homotopy Type Theory~\cite{univalent2013homotopy} enables transparent transport of proofs;
cubical type theory~\cite{cohen2016cubical} gives univalence a constructive interpretation. 

The problem that we solve is fundamentally about proof reuse,
which applies software reuse principles to ITPs. 
There is a wealth of work in proof reuse, from tactic languages~\cite{felty1994generalization} and logical frameworks~\cite{caplan1995logical},
to tools for proof abstraction and generalization~\cite{pons2000generalization, johnsen2004theorem},
to domain-specific methodologies~\cite{Delaware:2011:PLT:2048066.2048113} and frameworks~\cite{Delaware:2013:MLC:2429069.2429094}.

\toolnameb focuses on the specific problem of reuse
when adding fully-determined indices to types.
Other approaches to this problem include combinators which definitionally reduce to desirable terms~\cite{DBLP:journals/corr/abs-1803-08150} in the language Cedille,
and automatic generation of conversion functions in Ghostbuster~\cite{McDonell:2016:GTS:2951913.2951914} for GADTs in Haskell.
Our work focuses on a type theory different from both of these, in which the properties that allow for such combinators in Cedille are not present, and in which dependent types introduce challenges not present in Haskell.

\toolnameb is not the first tool to combine search with reuse. 
Optician~\cite{miltner2017synthesizing} synthesizes bidirectional string transformations;
a similar approach may help extend tooling to handle transformations for low-level data.
\textsc{sysname Patch}~\cite{ringer2018adapting} 
searches the difference in proofs for patches that can be used to repair proofs broken by changes;
\toolnameb uses a similar approach to identify functions
that form an equivalence. The resulting tools are complementary: \toolnameb supports the addition
of indices and hypotheses, which \textsc{sysname Patch} does not support; \textsc{sysname Patch} supports changes
in values, which \toolnameb does not support. 

\subsection*{Proof Evolution}

% From sysname PATCH, unchanged

There is a small body of work on change and dependency management for verification,
both to evaluate impact of potential changes and maximize reuse~\cite{873647, Autexier:2010:CMH:1986659.1986663}
and to optimize build performance~\cite{Celik:2017:IRP:3155562.3155588}.
These approaches may help isolate changes, which is necessary to identify future benchmarks, integrate
with CI systems, and fully support version updates.

\subsection*{Proof Refactoring}

% From sysname Pi, mostly unchanged

\textit{Refactoring} is the restructuring of code in a way that preserves semantics~\cite{opdyke1992refactoring};
more can be found in a survey~\cite{mens2004survey}.
\textit{Proof refactoring}~\cite{WhitesidePhD} is program refactoring for proofs.
The proof refactoring tool Levity~\cite{Bourke12} for Isabelle/HOL has seen large-scale industrial use.
Levity focuses on a different task: moving lemmas.
Chick~\cite{robert2018} and RefactorAgda~\cite{wibergh2019} are proof refactoring tools
in a Gallina-like language and in Agda, respectively.
% that also support a few changes that can be viewed as repairs~\cite{PGL-045}.
%Chick operates over a Gallina-like language, while RefactorAgda is implemented in Agda.
These tools support primarily syntactic changes and do not have tactic support.
% changes these tools support are still primarily syntactic,
%and neither of these tools have tactic support.

A few proof refactoring tools operate directly over tactics:
POLAR~\cite{Dietrich2013} refactors proof scripts in languages based on Isabelle/Isar~\cite{Wenzel2007isar},
CoqPIE~\cite{Roe2016} is an IDE with support for simple refactorings of Ltac scripts, and
Tactician~\cite{adams2015} is a refactoring tool for switching between tactics and tacticals.
This approach is not tractable for more complex changes~\cite{robert2018}.

\iffalse
\textit{Refactoring} is the restructuring of code in a way that preserves semantics~\cite{opdyke1992refactoring};
more can be found in a survey~\cite{mens2004survey}.
\textit{Proof refactoring} is refactoring in the setting of proof development~\cite{WhitesidePhD}.
Proof refactoring tools help automate this process, propogating a single change throughout the proof development.
Like program refactoring tools, proof refactoring tools can help keep developments
maintainable as they change over time~\cite{Bourke12}. In that way, it is possible to consider refactoring tools 
as both proactive and reactive.
Proof repair distinguishes itself from proof refactoring in that it considers changes that do not
preserve semantics, though the two concepts go hand-in-hand.
Proof refactoring tools can operate either at the level of proof automation or at the level of proof terms.

% TODO left off here

\paragraph{Refactoring Automation}

In proof assistants like Coq, proof engineers typically interact primarily with automation;
the details of proof terms are abstracted away. Accordingly, many proof refactoring tools
refactor automation directly, addressing challenges unique to proof engineering. % TODO like?
 % TODO adjust POLAR paragraph in survey paper to be more accurate
One such tool is \textsc{POLAR}~\cite{Dietrich2013}, a generic framework for proof refactoring. 
%\textsc{POLAR} is instantiated with two languages,
%both of which are based on the high-level proof language Isabelle/Isar~\cite{Wenzel2007isar}.
%Hiscript~\cite{WhitesidePhD}, a language with support for refactoring,
%and $\Omega$\textsc{script}~\cite{dietrich2011assertion}, a language with support for a proof search
%technique called proof planning~\cite{Bundy:1988:UEP:648228.752123}. % TODO in survey paper, write about separately in automation or something
Refactoring in \textsc{POLAR} works through a combination of rewrite rules that operate over
a graph representation of the underlying language. % TODO elaborate
\textsc{POLAR} implements ten refactorings by default,
and also supports the definition of custom refactorings by users. 
%The semantics-preserving guarantee that 
\textsc{POLAR} guarantees that all lemmas that go through before
the refactoring should continue to go though after the refactoring.
% TODO example refactoring, e.g. renaming a tactic, inc. what happens under the hood

Chick~\cite{robert2018front} includes a language of commands similar to Coq's commands for defining terms,
and can support some refactoring of definitions in that language;
while Chick focuses primarily on proof term refactoring, the Chick thesis
includes useful discussion of the challenges involved in refactoring Ltac proof scripts.

Some proof refactoring tools focus on specific refactoring tasks that are common in proof development.
For example, Levity~\cite{Bourke12} is a proof refactoring tool for an old version of Isabelle/HOL that automatically
moves lemmas %in such a way as 
to maximize reuse. The design of Levity is informed by
experiences with two large proof developments.
Tactician~\cite{adams2015} is a refactoring tool for proof scripts in HOL Light
that focuses on refactoring proofs between sequences of tactics and tacticals (tactic combinators like
the semicolon in Ltac).
%The Why3~\cite{Bobot2013} verification platform,
%a front-end which integrates several backend theorem provers and proof assistants including Coq, 
%includes functionality which automatically ports user tactics across sessions. 
% TODO is Why3 relevant? Also fix ref. in PE paper since it's confusing what Why3 actually does...
% TODO briefly describe what these are or how they work, example, etc; elaborate as much as possible, maybe email the dudes
% TODO does Levity belong here or in spec section?

% TODO left off here
Refactoring tools are often integrated into the user interfaces and integrated development environments (IDEs) % TODO is this the first reference of IDE?
that programmers use. To this spirit, the Coq IDE CoqPIE~\cite{Roe2016} %for Coq takes this approach for refactoring proof scripts.
includes a \textit{Replay} button which steps through the proof while renaming
any changed hypothesis names. 
CoqPIE can also automatically split out intermediate goals from a proof into separate lemmas. % TODO does this go in proof objects? blurry w/ vernac
There are plans to support more refactoring functionality in CoqPIE in the future. % TODO are any realized yet?
% TODO does CoqPIE support latest Coq? etc.

% TODO more citations maybe from related work

\paragraph{Refactoring Specifications and Proof Terms} % TODO again reconsider this terminology; maybe just terms and types everywhere

There is little work on refactoring proof terms in proof assistants based on dependent type theory like Coq.
This is the main focus of Chick~\cite{robert2018front}, which refactors terms in a dependently-typed functional language
similar to Gallina. To use Chick, the proof engineer applies some refactorings.
Chick then uses a program differencing algorithm to determine the changes to make elsewhere in the program,
then makes those changes.

Chick supports insertion, deletion, modification, and permutation of subterms.
While Chick presents itself as a proof refactoring tool,
this set of changes captures more than refactors.
Adding a new index to a type, for example, does not preserve the semantics of the original program.
The algorithms for differencing and application, however, are syntactic, and do not place any guarantees on
success and on the semantics of the modified program relative to the original program.
 % TODO really need to talk to Val more to get a sense for this and reread

\paragraph{Implications for Proof Repair}

Proof repair tools are similar to proof refactoring tools,
except that they support changes that do not preserve semantics; supporting
both of these is important. There is a lot to learn for proof repair from existing proof refactoring tools,
the most obvious of which is support for common
refactorings, which increases the reach of a proof repair tool and the incentive to use it.
%this is part of the research plan in Section~\ref{sec:plan}.
Likely refactorings worth supporting include hypothesis renaming (as in CoqPIE) and the refactorings Levity can perform, since
the need for both is justified by experiences with large-scale proof development.
%When studying the kinds of changes that proof engineers make, it may help to identify common classes
%of pure refactorings and support these in \sysname\ as well.
Another takeaway from proof refactoring tools is the possibility of supporting custom classes of refactorings or
repairs without requiring modification of the proof repair tool,
for example through custom commands or tactics.

A proof repair tool ideally ought to integrate naturally with proof engineers' workflows;
in Coq, this means integrating with automation.
% ideally, \sysname\ should handle changes not only
%to proof terms, but also to tactics. 
Proof refactoring tools that work at this level may offer hints for how to accomplish this for repairs as well.
The general case of this is not tractable since the semantics of Ltac are not well-defined,
but the updated Ltac2~\cite{ltac2}
may simplify this.
Also in line with supporting the typical workflow of proof engineers is IDE support,
as in CoqPIE; the way CoqPIE accomplishes this % TODO the particular button
may offer useful insights.

%Chick is the one proof refactoring tool that is not a pure refactoring tool.
Since Chick handles some semantic changes, it is possible to think of Chick as a proof repair tool like \sysname.
Chick was developed in parallel to and in cooperation with \sysname, and has a number of similarities.
The Chick workflow is similar to the \sysname\ workflow:
Like \sysname, Chick takes a set of example changes (in this case, supplied by the programmer),
and uses a program differencing algorithm to determine the changes to make elsewhere in the program.

Chick also has some clear differences from \sysname, aside from operating over the Chick language instead of Gallina itself.
Unlike early versions of \sysname, Chick also applies the changes it finds to derive the new program.
However, Chick does this using a syntactic algorithm that handles simple transformations and does
not provide guarantees on the output; for some changes to inductive types, for example, not all proofs from
the old program are preserved in the new program, and the algorithm may fail % TODO does it
on this class of changes. \devoid\ focuses on applying these changes for a class of changes like this
with semantic guarantess. % obviously rephrase
It may be possible to integrate the techniques from Chick, \sysname, and \devoid\
into the same tool and use them to advance the state of proof repair.
%use the techniques from Chick when improving \sysname\
%and handle a larger class of changes, and take advantage of some of the differences between the tool.
\fi

\subsection*{Proof Design}

% From sysname Pi, with some sysname PATCH related work integrated:

Much work focuses on designing proofs
to be robust to change, rather than fixing broken proofs.
This can take the form of design principles, like using 
information hiding techniques~\cite{Woos:2016:PCF:2854065.2854081, Klein:2014:CFV:2584468.2560537}
or any of the structures~\cite{Chrzaszcz2003, Sozeau2008, Saibi:PhD} for encoding interfaces in Coq.
CertiKOS~\cite{certikos} introduces the idea of a deep specification to ease verification of large systems.
Design principles for specific domains (like formal metatheory~\cite{Aydemir2008, Delaware2013POPL, Delaware2013ICFP})
can also make verification more tractable.
Design and repair are complementary: design requires foresight, while repair can occur retroactively.
Repair can help with changes that occur outside of the proof engineer's control,
or with changes that are difficult to protect against even with informed design.

Another approach to this is to use heavy proof automation, for example through
program-specific proof automation~\cite{Chlipala:2013:CPD:2584504}
%implementations of decision procedures~\cite{Pugh1991},
or general-purpose hammers~\cite{Blanchette2016b, Blanchette2013, Kaliszyk2014, Czajka2018}.
The degree to which proof engineers rely on automation varies, as seen in the data from a user study~\cite{replica}.
Automation-heavy proof engineering styles localize the burden of change to the automation,
but can result in terms that are large and slow to type check,
and tactics that can be difficult to debug.
While these approaches are complementary, more work is needed for \toolname to better support 
developments in this style.

\subsection*{Proof Automation}

% From sysname PATCH, unchanged:

We address a missed opportunity in proof automation for ITP: searching
for patches that can fix broken proofs.
This is complementary to existing automation techniques. Nonetheless, there is a wealth
of work in proof automation that makes proofs more resilient to change.
Powerful tactics like \lstinline{crush}~\cite{chlipala:cpdt} can make
proofs more resilient to changes. 
Hammers like Isabelle's sledgehammer~\cite{Blanchette2013} can make proofs agnostic to some low-level changes.
Recent work~\cite{coqhammer} paves the way for a hammer in Coq.
Even the most powerful tactics cannot address all changes;
our hope is to open more possibilities for automation.

Powerful project-specific tactics~\cite{chlipala:cpdt, Chlipala2013} can help prevent low-level maintenance tasks.
Writing these tactics requires good engineering~\cite{Gonthier2011} and domain-specific knowledge,
and these tactics still sometimes break in the face of change.
A future patching tool may be able to repair tactics; the debugging process
for adapting a tactic is not too dissimilar to providing an example to a tool.

Rippling~\cite{rippling} is a technique for automating inductive proofs that uses restricted rewrite rules to
guide the inductive hypothesis toward the conclusion; this may guide improvements to the
differencing, abstraction, and specialization components.
The abstraction and factoring components address specific classes of unification problems;
recent developments to higher-order unification~\cite{Miller:2012:PHL:2331097} may help
improve these components.
Lean~\cite{selsam:lean} introduces the first congruence closure algorithm for dependent type theory that
relies only on the Uniqueness of Identity Proofs (UIP) axiom. While UIP is not fundamental to Coq,
it is frequently assumed as an axiom; when it is, it may be tractable to use a similar algorithm to improve the tool.

GALILEO~\cite{bundyreasoning} repairs faulty physics theories
in the context of a classical higher-order logic (HOL); there is preliminary work extending this
style of repair to mathematical proofs. 
Knowledge-sharing methods~\cite{tgck-cicm14} can adapt some proofs across different representations of HOL.
These complementary approaches may guide extensions to support decidable domains and classical logics.


