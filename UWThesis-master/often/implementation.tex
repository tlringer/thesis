\section{Implementation}
\label{sec:pi-implementation}

Like the \contour[2]{violet}{\kl{\sysname}} prototype,
\contour[2]{violet}{\kl{\toolnamec}} is also included in the \contour[2]{violet}{\kl{\sysnamelong}} \kl{plugin} suite by default.
As with \sysname, the latest version supports Coq 8.8, with Coq 8.9.1 support in a branch.
This section describes the implementation of the core functionality of the \toolnamec plugin (Section~\ref{sec:pi-details}),
along with important features for workflow integration (Section~\ref{sec:pi-workflow}).
The interested reader can follow along in the repository.

\subsection{Tool Details}
\label{sec:pi-details}

% PUMPKIN Pi, unchanged
The implementation (Section~\ref{sec:pi-details-command}) of the \lstinline{Repair} command
looks up the configuration for the types, invoking a search procedure for automatic configuration if relevant.
The configuration, whether obtained manually or automatically, is cached for future calls to \lstinline{Repair}.
The implementations of the differencing search procedures for automatic configuration (Section~\ref{sec:pi-details-diff})
have freedom over the details of how they are implemented, as long as they return configurations in the end.
The transformation (Section~\ref{sec:pi-details-trans}) operates directly over proof terms in Gallina, with the cached configuration
also defined as proof terms in Gallina.
As with \sysname, the \toolnamec extension to \sysnamelong does not extend the \kl{TCB} in any way (Section~\ref{sec:pi-tcb}).

\subsubsection{The Command}
\label{sec:pi-details-command}

The implementation of \toolnamec exposes the repair workflow
to proof engineers through the \lstinline{Repair} command. % TODO link to code
This invokes the workflow from Figure~\ref{fig:system} on page~\pageref{fig:system}.
When the proof engineer invokes the \kl{command}:

\begin{lstlisting}
  Repair A B in old_proof as new_proof.
\end{lstlisting}
The first step---\textsc{Configure}---looks up \Aa and \B in a cache of configurations.
If it finds one, it uses that; otherwise, it runs the differencing search procedures for \kl{automatic configuration}.
If that fails, it prompts the proof engineer to supply a configuration \kl{manually}.
Proof engineers can supply manual configurations by defining them as Gallina terms and
providing them to the \lstinline{Configure Repair} command.

Once \toolnamec has found a configuration, the next step---\textsc{Transform}---runs the transformation.
It transforms \lstinline{old_proof} into some new proof term, then defines the new proof term as \lstinline{new_proof}.
It is also possible to ask \toolnamec to automatically generate a name. % TODO is it?
The final step---\textsc{Decompile}---runs in the end to suggest a proof script.

There are a few other useful commands in \toolnamec that are detailed in the repository.
Notably, the \lstinline{Repair Module} command uses higher-order functions written by \kl{Nate}
to operate over entire modules at once and define new, repaired versions of those modules.
The \lstinline{Preprocess} command converts functions to use eliminators, so that the transformation
can assume \kl{primitive eliminators}.
The \lstinline{Lift} and \lstinline{Lift Module} commands skip the decompilation step,
when only the term is desired in the end.
Several options make it possible to control whether \toolnamec generates correctness proofs,
as well as how aggressively it runs optimizations, and what it considers in its optimizations.
The repository includes more information on all of these.

\subsubsection{Differencing}
\label{sec:pi-details-diff}

As mentioned in Section~\ref{sec:proc}, \kl{differencing} inside of
\toolnamec implements four search procedures for automatic configuration~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftconfig.ml}{\circled{6}}:

\begin{enumerate}
\item algebraic ornaments,
\item unpacking $\Sigma$ types,
\item swapping constructors, and
\item moving between nested pairs and records.
\end{enumerate}
I already detailed the first of these in Section~\ref{sec:proc}.
The second is often used in combination with the first, to get from equivalences like:

\begin{lstlisting}
  $A \simeq \Sigma (i : I) . A_I\ i$
\end{lstlisting}
to equivalences like:

\begin{lstlisting}
  $\Sigma (a : A) . \pi\ a = i \simeq A_I\ i$
\end{lstlisting}
where $\pi$ is the indexer.
The first two configurations are used in combination, for example,
to repair proofs in response to the change from lists to vectors of a particular length in Figure~\ref{fig:listtovect},
as long as the proof engineer proves the missing length invariant.
This differencing algorithm is implemented by simply instantiating the types \Aa and \B of a generic 
configuration that can be defined inside of Coq directly.

The third configuration---invoked in the example in Section~\ref{sec:overview}---constructs a swap map
that maps the constructors of \Aa to the swapped constructors of \B, then otherwise runs an algorithm much like
the algorithm for algebraic ornaments.
When there are multiple possible swap maps, it returns an ordered list of possible mappings to choose from,
and prompts the proof engineer to select one. % TODO link, here and elsewhere...
It also lets the proof engineer supply the swap map directly, and from that can derive the configuration, the equivalence, and the proof.
The fourth search procedure is similar to the search procedure for algebraic ornaments, but with some additional logic
to handle nested tuples.

All search procedures have the historical detail of defining the equivalence first, then deriving the configuration from it,
rather than the other way around.
For simplicity, the search procedures currently implemented inside of \toolnamec also make some syntactic assumptions
about the input and output types.
The details of these restrictions can be found inside of the repository. % TODO link

Defining new search procedures comes with a lot of freedom, as long as the search procedures produce a configuration in the end.
I found it simple to add configuration four in a matter of a few days in response to a request by an industrial proof engineer,
reusing existing functions defined in other search procedures.
I would not expect the average proof engineer to be able to do the same, and I have not yet asked anyone else to write a search procedure
to gauge the effort involved for others.
It is notable that manual configuration is always an option when search procedures do not exist,
but the results in Section~\ref{sec:pi-results} suggest that automatic configuration is very helpful for saving developer effort,
and so worth implementing for useful classes of equivalences.

\subsubsection{Transformation}
\label{sec:pi-details-trans}

The implementation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/lift.ml}{\circled{4}}
of the \kl{transformation} from Section~\ref{sec:pi-trans} operates over terms in Gallina.
It takes as input two types \Aa and \B, along with a configuration that induces an equivalence between them.
\Aa and \B may not necessarily be the inputs to differencing---in the case of algebraic ornaments, for example,
differencing takes \Aa and \AI as inputs, but the transformation takes \Aa and \B as inputs,
where \B is $\Sigma (i : I).A_I i$.

The unification heuristics run before the transformation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftconfig.ml}{\circled{6}},
and in the end return which transformation derivation to run,
and with which arguments~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftrules.ml}{\circled{12}}.
For the most part, the derivations are implemented in a way that corresponds to the derivations in Figure~\ref{fig:final}
on page~\pageref{fig:final}, but with a few differences highlighted below.

\paragraph{From CIC$_{\omega}$ to Gallina}
The implementation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/lift.ml}{\circled{4}} % lift.ml
of the transformation handles language differences to scale from CIC$_{\omega}$ to Gallina.
For example, recall that the transformation assumes \kl{primitive eliminators}, while Gallina implements eliminators in terms of pattern matching and fixpoints. 
To handle terms that use these features, \kl{Nate} implemented a \lstinline{Preprocess} command in \toolnamec that 
translates these terms into corresponding eliminator applications.
This command can preprocess a definition (like \lstinline{length} from Figure~\ref{fig:length} on page~\pageref{fig:length}) or an entire module
(like \lstinline{List}, as shown in \href{http://github.com/uwplse/ornamental-search/blob/itp+equiv/plugin/coq/examples/ListToVect.v}{\lstinline{ListToVect.v}}) for lifting. % TODO update links
It currently supports fixpoints that are structurally recursive on only 
immediate substructures.\footnote{This is enough to preprocess many practical terms, including the entire \lstinline{List} module.
But it is not as general as it could be~\cite{recursion-elimination, jesper}.
A more general translation may help \toolnamec support more terms, and discussions
with Coq developers a couple of years ago suggest that the implementation of such a translation
building on work from the equations~\cite{sozeau:equations} plugin was in progress.
I do not know the current status of that project.}
To translate such a fixpoint, it first extracts a motive, then generates each case
by partially reducing the function's body under a hypothetical context for the constructor arguments.
The implementation documents this and other language differences.

\paragraph{Optimizations}
The implementation of the transformation includes a number of cases that correspond to optimizations
not found in the original transformation.
For example, the transformation assumes that all terms are fully \smallmath{$\eta$}-expanded. Sometimes,
however, \smallmath{$\eta$}-expansion is not necessary.
For efficiency, rather than fully \smallmath{$\eta$}-expand ahead of time, \toolnamec \smallmath{$\eta$}-expands lazily, 
only when it is necessary for correctness.
The \lstinline{LazyEta} optimization implements this optimization.
The code denotes all optimizations explicitly~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/lift.ml}{\circled{4}}
and explains them in detail in comments~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftrules.ml}{\circled{12}}.
Section~\ref{sec:engineers} describes some other optimizations included for the sake of integration with proof engineering workflows.

\paragraph{Termination}
When a subterm unifies with a configuration term, this suggests that \toolnamec \textit{can}
transform the subterm, but it does not necessarily mean that it \textit{should}.
In some cases, doing so would result in nontermination.
For example, if \B is a refinement of \Aa, then the transformation can always run \textsc{Equivalence}
over and over again, forever.
%\textsc{Devoid} ruled out this case by simply prohibiting the case where \B refers to \A, but we found it sometimes
%useful to support this case.
\toolnamec thus includes some simple termination checks in the code~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftrules.ml}{\circled{12}}. % liftrules.ml

\paragraph{Intent}
Even when termination is guaranteed, whether to transform a subterm depends on the proof engineer's intent.
That is, \toolnamec automates the case of porting \textit{every} \Aa to \B,
but proof engineers sometimes wish to port only \textit{some} $A$s to $B$s.
\toolnamec has some support for this using an interactive workflow~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/minimal_records.v}{\circled{13}},
with plans for automatic support in the future. % minimal_records.v, but show this
%We helped the proof engineer do this by interacting with \toolnamec using a particular workflow.

\subsubsection{Trusted Computing Base}
\label{sec:pi-tcb}

As with \sysname, \toolnamec is implemented as a Coq plugin, and produces terms that Coq type checks in the end.
\toolnamec does not modify the type checker, and furthermore does not add any axioms, so it does not increase the \kl{TCB}.
This is perhaps even more notable for \toolnamec, since all other implementations of transport across equivalences
that I am aware of at least sometimes add axioms.
Since \toolnamec implements transport as a proof term transformation, it is able to circumvent this and not increase the TCB in any way.

\subsection{Workflow Integration}
\label{sec:pi-workflow}

% PUMPKIN Pi Decompiler & impl
So far, I have described the core functionality of \toolnamec.
But \toolnamec has many additional features for the sake of integration with typical proof engineering workflows.
Most notable of these is the decompiler from Gallina to Ltac (Section~\ref{sec:decompiler}),
which helps \toolnamec produce a suggested proof script that the proof engineer can maintain in the end.
This and other features help \toolnamec reach real proof engineers (Section~\ref{sec:engineers}).

\subsubsection{Decompiling to Tactics}
\label{sec:decompiler}

\textbf{Transform} produces a \kl{proof term},
while the proof engineer typically writes and maintains \kl{proof scripts} made up of \kl{tactics}.
\toolnamec improves usability thanks to the realization that, since Coq's proof term language \kl{Gallina} is very structured,
it is possible to decompile these Gallina terms to suggested \kl{Ltac} proof scripts for the proof engineer 
to maintain.

%\footnote{I advised \kl{RanDair} on this project. RanDair implemented the decompiler.
%My role was advising the design of the decompiler, helping with the formalization of the a small core part of the decompiler, and
%integrating it into the rest of \toolnamec.} % TODO move this

%\begin{quote}
%\textbf{Insight 3}: The transformed proof terms can then be translated back to tactic scripts.
%\end{quote}

\textbf{Decompile} implements a prototype of this translation~\href{https://github.com/uwplse/coq-plugin-lib/tree/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}: % decompiler.ml
it translates a proof term to a suggested proof script that attempts to prove the same theorem 
the same way.
Note that this problem is not well defined: while there is always a proof script that 
works (applying the proof term with the \lstinline{apply} tactic), the result is often qualitatively unreadable.
This is the baseline behavior to which the decompiler defaults.
The goal of the decompiler is to improve on that baseline as much as possible,
or else suggest a proof script that is close enough to correct that the proof engineer can
manually massage it into something that works and is maintainable.

\textbf{Decompile} achieves this in two passes: The first pass decompiles proof terms to proof scripts that use a predefined set of tactics.
The second pass improves on suggested tactics by simplifying arguments, substituting tacticals, and using
hints like custom tactics and decision procedures. % TODO not quite true anymore? IDK what to do about that, though.

\paragraph{First Pass: Basic Proof Scripts}
The first pass takes Coq terms and produces tactics in Ltac, the proof script language for Coq.
Ltac can be confusing to reason about, since Ltac tactics can refer to Gallina terms, and the semantics of Ltac depends both on the
semantics of Gallina and on the implementation of proof search procedures written in OCaml.
To give a sense of how the first pass works without the clutter of these details, I start by defining a mini decompiler that 
implements a simplified version of the first pass.
I then explain how \kl{RanDair} scaled this to the implementation.

The mini decompiler takes CIC$_{\omega}$ terms and produces tactics in a 
mini version of Ltac which I call Qtac.
The syntax for Qtac is in Figure~\ref{fig:ltacsyntax1}.
Qtac includes hypothesis introduction (\lstinline{intro}),
rewriting (\lstinline{rewrite}), symmetry of equality (\lstinline{symmetry}),
application of a term to prove the goal (\lstinline{apply}), induction (\lstinline{induction}),
case splitting of conjunctions (\lstinline{split}),
constructors of disjunctions (\lstinline{left} and \lstinline{right}), and
composition (\lstinline{.}).
Unlike in Ltac, \lstinline{induction} and \lstinline{rewrite} take a motive explicitly (rather than relying on unification),
and \lstinline{apply} creates a new subgoal for each function argument.
%The implementation reasons about Ltac and so does not make these assumptions.


\begin{figure*}
\small
\begin{grammar}
<v> $\in$ Vars, <t> $\in$ CIC$_{\omega}$

<p> ::= intro <v> \\ | rewrite <t> <t> \\  |  symmetry \\  |  apply <t> \\  |  induction <t> <t> \{ <p>, \ldots, <p> \} \\  |  split \{ <p>, <p> \} \\ |  left \\  |  right \\  |  <p> . <p>
\end{grammar}
\vspace{-0.5cm}
\caption{Qtac syntax.}
\label{fig:ltacsyntax1}
\end{figure*}

\begin{figure*}
\begin{mathpar}
\mprset{flushleft}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $t$ $\Rightarrow$ $p$}\vspace{-0.5cm}\\

\inferrule[Intro]
  { \Gamma,\ n : T \vdash b \Rightarrow p }
  { \Gamma \vdash \lambda (n : T) . b \Rightarrow \mathrm{intro}\ n.\ p }

\inferrule[Symmetry]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathtt{eq\_sym}\ H \Rightarrow \mathrm{symmetry}.\ p }

\inferrule[Split]
  { \Gamma \vdash l \Rightarrow p \\ \Gamma \vdash r \Rightarrow q }
  { \Gamma \vdash \mathrm{Constr}(0,\ \wedge)\ l\ r \Rightarrow \mathrm{split} \{ p, q \}.\ }\\

\inferrule[Left]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(0,\ \vee)\ H \Rightarrow \mathrm{left}.\ p }

\inferrule[Right]
  { \Gamma \vdash H \Rightarrow p }
  { \Gamma \vdash \mathrm{Constr}(1,\ \vee)\ H \Rightarrow \mathrm{right}.\ p }

\inferrule[Rewrite]
  { \Gamma \vdash H_1 : x = y \\ \Gamma \vdash H_2 \Rightarrow p }
  { \Gamma \vdash \mathrm{Elim}(H_1,\ P) \{ x,\ H_2,\ y \} \Rightarrow \mathrm{symmetry}.\ \mathrm{rewrite}\ P\ H_1.\ p }\\

\inferrule[Induction]
  { \Gamma \vdash \vec{f} \Rightarrow \vec{p} }
  { \Gamma \vdash \mathrm{Elim}(t,\ P)\ \vec{f} \Rightarrow \mathrm{induction}\ P\ t\ \vec{p} }

\inferrule[Apply]
  { \Gamma \vdash t \Rightarrow p }
  { \Gamma \vdash f t \Rightarrow \mathrm{apply}\ f.\ p }

\inferrule[Base]
  { \\ }
  { \Gamma \vdash t \Rightarrow \mathrm{apply}\ t }
\end{mathpar}
\caption{Qtac decompiler semantics.}
\label{fig:someantics}
\end{figure*}

The semantics for the mini decompiler $\Gamma \vdash t \Rightarrow p$ are in Figure~\ref{fig:someantics} (assuming $=$, \lstinline{eq_sym}, $\wedge$, and $\vee$ are defined as in Coq).
As with the real decompiler, the mini decompiler defaults to the proof script
that applies the entire proof term with \lstinline{apply} (\textsc{Base}).
Otherwise, it improves on that behavior by recursing over the proof term and constructing a proof script using a predefined set of tactics.

\iffalse
\begin{figure*}
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>(@\vspace{-0.04cm}@)
  (@\codesima{list\_rect}@) _ _  (fun (@\codesima{a l H}@) =>(@\vspace{-0.04cm}@)
    (@\codesimc{eq\_ind\_r}@) _ (@\codesimd{eq\_refl}@) (@\codesimc{(app\_nil\_r (rev l) (a::[]))}@))(@\vspace{-0.04cm}@)
    (@\codesime{eq\_refl}@)(@\vspace{-0.04cm}@)
    (@\codesima{y0}@)(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
(@\vspace{-0.14cm}@)
- (@\codesimb{intro y0.}@) (@\codesima{induction y0 as [a l H|].}@)(@\vspace{-0.04cm}@)
  + (@\codesimc{simpl. rewrite app\_nil\_r.}@) (@\codesimd{auto.}@)(@\vspace{-0.04cm}@)
  + (@\codesime{auto.}@)(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
\end{lstlisting}
\end{minipage}
\vspace{-0.3cm}
\caption{Proof term (left) and decompiled proof script (right) for the base case of 
\lstinline{rev_app_distr} (Section~\ref{sec:overview}),  with corresponding terms and tactics 
grouped by color and number.}
\label{fig:rainbow}
\end{figure*}
\fi

For the mini decompiler, this is straightforward: Lambda terms become introduction (\textsc{Intro}).
Applications of \lstinline{eq_sym} become symmetry of equality (\textsc{Symmetry}).
Constructors of conjunction and disjunction map to the respective tactics (\textsc{Split}, \textsc{Left}, and \textsc{Right}).
Applications of equality eliminators compose symmetry (to orient the rewrite direction) with rewrites (\textsc{Rewrite}),
and all other applications of eliminators become induction (\textsc{Induction}).
The remaining applications become apply tactics (\textsc{Apply}).
In all cases, the decompiler recurses, breaking into cases, until only the \textsc{Base}
case holds. % at which point we are done.

While the mini decompiler is very simple, only a few small changes were needed for \kl{RanDair}
to move this to Coq.
%The result can already handle some of the example proofs \toolnamec has produced.
The generated proof term of \lstinline{rev_app_distr} from Section~\ref{sec:overview},
for example, consists only of induction, rewriting, simplification, and reflexivity (solved by \lstinline{auto}).
Figure~\ref{fig:rainbow} shows the proof term for the base case of \lstinline{rev_app_distr} 
alongside the proof script that \toolnamec suggests.
This script is fairly low-level and close to the proof term, but it is already something that the proof engineer
can step through to understand, modify, and maintain.
There are few differences from the mini decompiler needed to produce this,
for example handling of rewrites in both directions (\lstinline{eq_ind_r} as opposed to \lstinline{eq_ind}),
simplifying rewrites,
and turning applications of \lstinline{eq_refl} into \lstinline{reflexivity} or \lstinline{auto}.

\paragraph{Second Pass: Better Proof Scripts}
The implementation of \textbf{Decompile} first runs something similar to the mini decompiler, then modifies the suggested tactics to produce a more natural proof script~\href{https://github.com/uwplse/coq-plugin-lib/tree/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}. % decompiler.ml
For example, it cancels out sequences of \lstinline{intros} and \lstinline{revert},
inserts semicolons, and removes extra arguments to \lstinline{apply} and \lstinline{rewrite}. %, ensuring the result still holds. % TODO rewrite
It can also take tactics from the proof engineer (like part of the old proof script) as hints,
then iteratively replace tactics with those hints, checking for correctness.
This makes it possible for suggested scripts to include custom tactics and decision procedures.

\begin{figure}
\begin{lstlisting}
fun (@\codesimb{(y0 : list A)}@) =>(@\vspace{-0.04cm}@)
  (@\codesima{list\_rect}@) _ _  (fun (@\codesima{a l H}@) =>(@\vspace{-0.04cm}@)
    (@\codesimc{eq\_ind\_r}@) _ (@\codesimd{eq\_refl}@) (@\codesimc{(app\_nil\_r (rev l) (a::[]))}@))(@\vspace{-0.04cm}@)
    (@\codesime{eq\_refl}@)(@\vspace{-0.04cm}@)
    (@\codesima{y0}@)(@\vspace{-0.04cm}@)
(@\vspace{-0.04cm}@)
- (@\codesimb{intro y0.}@) (@\codesima{induction y0 as [a l H|].}@)(@\vspace{-0.04cm}@)
  + (@\codesimc{simpl. rewrite app\_nil\_r.}@) (@\codesimd{auto.}@)(@\vspace{-0.04cm}@)
  + (@\codesime{auto.}@)(@\vspace{-0.04cm}@)
\end{lstlisting}
\vspace{-0.3cm}
\caption{Proof term (top) and decompiled proof script (bottom) for the base case of 
\lstinline{rev_app_distr} (Section~\ref{sec:overview}), with corresponding terms and tactics 
grouped by color \& number.}
\label{fig:rainbow}
\end{figure}

\paragraph{From Qtac to Ltac}
The mini decompiler assumes more predictable versions of \lstinline{rewrite} and \lstinline{induction}
than those in Coq. \textbf{Decompile} includes additional logic to reason about these tactics~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}. % decompiler.ml
For example, Qtac assumes that there is only one \lstinline{rewrite} direction. Ltac has two rewrite directions,
and so the decompiler infers the direction from the motive.

Qtac also assumes that both tactics take the \kl{motive} explicitly,
while in Coq, both tactics infer the motive automatically.
Consequentially, Coq sometimes fails to infer the correct motive.
% infers the wrong motive, % without manipulation of goals and hypotheses,
%or fails to infer a motive at all.
%This is especially common for \lstinline{rewrite}, which is purely syntactic.
To handle induction, the decompiler strategically uses \lstinline{revert} to manipulate the goal
so that Coq can better infer the motive.
To handle rewrites, it uses \lstinline{simpl} to simplify the goal before rewriting.
Neither of these approaches is guaranteed to work, so the proof engineer may sometimes need to tweak the 
suggested proof script appropriately.
Even if \kl{RanDair} passes Coq's induction principle an explicit motive, Coq still sometimes fails due
to unrepresented assumptions.
Long term, using another tactic like \lstinline{change} or \lstinline{refine} before applying these tactics
may help with cases for which Coq cannot infer the correct motive.

\paragraph{From CIC$_{\omega}$ to Gallina}
Scaling the decompiler to Gallina introduces let bindings, which are generated by 
tactics like \lstinline{rewrite in}, \lstinline{apply in}, and \lstinline{pose}.
\textbf{Decompile} implements~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}} % decompiler.ml
support for \lstinline{rewrite in} and \lstinline{apply in} similarly to how it supports
\lstinline{rewrite} and \lstinline{apply}, except that it ensures that the unmanipulated hypothesis does not occur in the body of the let expression,
it swaps the direction of the rewrite, and it recurses into any generated subgoals.
In all other cases, it uses \lstinline{pose}, a catch-all for let bindings.

\paragraph{Forfeiting Soundness}
While there is a way to always produce a correct proof script,
by default, \textbf{Decompile} deliberately forfeits soundness to suggest more useful tactics.
For example, it may suggest the \lstinline{induction} tactic, but leave motive inference to the proof engineer.
I have found these suggested tactics easier to work with (Section~\ref{sec:pi-results}).
Note that in the case the suggested proof script is not quite correct,
it is still possible to use the generated proof term directly.
Still, in the event that soundness is desirable, \kl{RanDair} has since implemented a simplified sound
version of the decompiler in a branch. % TODO link
%Work in progress is integrating this with a machine learning tool for friendlier suggested proof scripts
%that preserve soundness. TODO if we don't mention elsewhere, mention here

\paragraph{Pretty Printing}
After decompiling proof terms, \textbf{Decompile} pretty prints the result~\href{https://github.com/uwplse/coq-plugin-lib/blob/9ef05815c261de9c99b604c6b581ba1c4fbc1a46/src/coq/decompiler/decompiler.ml}{\circled{11}}.
Like the mini decompiler, \textbf{Decompile} represents its output using a predefined grammar of Ltac tactics,
albeit one that is larger than Qtac, and that also includes tacticals.
It maintains the recursive proof structure for formatting. %, then uses that to print proofs of subgoals using bullet points.
%It displays the resulting proof script to the proof engineer, who can modify it as needed.
%It includes scripts that automate the process of printing all of these tactic proofs to a Coq file,
%in case the proof engineer does not want an interactive workflow.
\toolnamec keeps all output terms from \textbf{Transform} in the Coq environment in case the decompiler does not succeed.
Once the proof engineer has the new proof, she can remove the old one.

\subsubsection{Reaching Real Proof Engineers}
\label{sec:engineers}

The goal of workflow integration is to reach real proof engineers.
Many of my design decisions in implementing \toolnamec were informed by my partnership with
an industrial proof engineer (Section~\ref{sec:pi-results}).
For example, the proof engineer rarely had the patience to wait more than ten seconds
for \toolnamec to port a term,
so I implemented optional aggressive caching, even caching intermediate subterms
encountered while running the transformation~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/cache/caching.ml}{\circled{14}}. % TODO caching.ml
I also added a cache to tell \toolnamec not to $\delta$-reduce certain terms~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/cache/caching.ml}{\circled{14}}.
With these caches, the proof engineer found \toolnamec efficient enough to use on a code base with tens of thousands of lines of code and proof.

 % caching.ml
%These caches are implemented in \href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/src/cache/caching.ml}{caching.ml}.
%or recurse into certain modules.
% set certain terms or modules as opaque to \toolnamec, to prevent unnecessary $\delta$-reduction


The experiences of proof engineers also inspired new features.
For example, I implemented a search procedure to generate custom eliminators %(\href{https://github.com/uwplse/pumpkin-pi/blob/master/plugin/src/automation/search/smartelim.ml}{smartelim.ml})
to help reason about types like $\Sigma$\lstinline{(l : list T).length l = n}
by reasoning separately about the projections~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/search/smartelim.ml}{\circled{15}}. %smartelim.ml
I added informative error messages~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/lib/ornerrors.ml}{\circled{22}} to help the proof engineer distinguish between user errors and bugs. % TODO link to errors
\kl{Nate} implemented machinery for whole module processing to handle entire libraries at once.
These features helped with workflow integration. % tactic decompiler helped with integration into proof engineering workflows.

% TODO maybe others? IDK
