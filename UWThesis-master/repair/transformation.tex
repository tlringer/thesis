\section{Transformation}
\label{sec:pumpkin-trans}

The proof term transformations together transform a patch candidate into a reusable proof patch.
At a high level, these transformations adapt the candidate to the context of the goal type that \sysname infers.
As with differencing, the transformations are aware of and guided by the semantics of Gallina's type theory CIC$_{\omega}$.
This section describes the design~\ref{sec:pumpkin-trans-design} and limitations~\ref{sec:pumpkin-trans-limitations} of these transformations.
Section~\ref{sec:pumpkin-impl-trans} describes the implementation in \sysname.

% TODO could write more probably

\subsection{Design}
\label{sec:pumpkin-trans-design}

% TODO honestly this should go in spec then? both of these in each section? and just say we don't show too much here since prototype?
The transformations together recurse over the structure of each term in the list of candidates $\vec{t}$ in an environment $\Gamma$,
and adapt that candidate to some new context in a goal-direct manner.
In the end, if successful, they produce a reusable proof patch $p$ with type $G$, where $G$ is the inferred goal type.
That is, we can view the high-level composition of transformations as a single judgment
$\Gamma\ \vdash\ (\vec{t},\ G)\ \Downarrow_{t} p$, where in the end, $\Gamma \vdash p : G$. 

The details vary by transformation,
and the details of which transformations run at all and in what order to reach the goal vary by configuration.
For historical reasons, as \sysname was a prototype, these tranformations are not formalized. % TODO formalize if time
Here I describe at a high level the design of each of the four transformations in CIC${_\omega}$.
Section~\ref{sec:pumpkin-impl-trans} describes additional features needed for implementation in Gallina,
and the \toolnamec extension in Chapter~\ref{chapt:pi} formalizes a more elegant proof term transformation building on some of the insights from the \sysname transformation prototypes.

% From Motivating the Core and Implementation

\paragraph{Specialization}
Sometimes, patch candidates are too general.
Specialization takes a candidate that is too general,
and specializes it to specific arguments as determined by the difference in terms.
To find a patch for Figure~\ref{fig:example}, for example, \sysname
specialized the patch candidate to \lstinline{p} to produce the final patch.

Specialization takes a single patch candidate, some arguments, and a reduction strategy, and returns a new candidate.
It first applies the function to the argument, then applies the reduction strategy on the result.
The default reducer, for example, uses $\beta\iota$-reduction in Coq~\cite{equality}.
Section~\ref{sec:pumpkin-impl-trans} decribes other reducers.
The only requirement for a reducer is that the end result should be definitionally equal to the original.

Depending on the configuration and the step in the process, the transformed candidate may be the reusable patch,
or it may just be an intermediate candidate.
It is the job of the patch finding procedure to provide both the candidate and the arguments,
and to determine which transformation to run next, if applicable.

\iffalse
It works by combining application with reduction, with just one derivation:

\begin{mathpar}
\small
\hfill\fbox{$\Gamma$ $\vdash$ $(t,\ \vec{a},\ \Downarrow{r}) \Downarrow_{s} t'$ }\\

\inferrule[Specialize]
  { \Gamma \vdash t\ \vec{a} \Downarrow_{r} t'}
  { \Gamma \vdash (t,\ \vec{a},\ \Downarrow_{r}) \Downarrow_{s} t'}
\end{mathpar}
There is just one super weird thing I do not even know if I am allowed to do here:
the reduction strategy $\Downarrow_{r}$ is itself a transformation that reduces a term in an arbitrary way.
\fi

\paragraph{Generalization} In other cases, a patch candidate is too specific.
Generalization takes a candidate that is too specific and generalizes it.
We saw this for the example in e~\ref{fig:example} as well:
to go from the candidate that \sysname found in the base case to the eventual reusable patch,
\sysname generalized the candidate by the argument \lstinline{m} (before applying specialization).

There are two kinds of generalization.
The first generalizes candidates that map between types that share a common argument, like: % TODO need to explain the arrow syntax and assume we can do it for CIC somewhere

\begin{lstlisting}[language=coq]
  $Q\ t\ \rightarrow P\ t$
\end{lstlisting}
to abstract the common argument:

\begin{lstlisting}[language=coq]
  $Pi (\diff{t0} : T),\ Q\ \diff{t0} \rightarrow P\ \diff{t0}$
\end{lstlisting}
where $T$ is the type of $t$.
The second generalizes candidates that map between types that share a common function, like:

\begin{lstlisting}[language=coq]
  $P\ t'\ \rightarrow P\ t$
\end{lstlisting}
to abstract the common function:

\begin{lstlisting}[language=coq]
  $Pi (\diff{P0} : T),\ \diff{P0} t' \rightarrow \diff{P0} t$
\end{lstlisting} 
where $T$ is the type of $P0$.

Generalization takes a patch candidate, the goal type, and the function arguments or function to abstract.
It first wraps the candidate inside of a lambda from the type of the term to abstract.
Then, it substitutes terms inside the body with the abstract term.
It continues to do this until there is nothing left to generalize, then filters results by the goal type.
Consider, for example, abstracting this candidate by \lstinline{m}: % TODO note it's from the thing

\begin{lstlisting}[language=coq]
  $\lambda (H : n <= m) . \mathrm{le_plus_trans}\ n\ m\ (S O)\ H$
  $: n <= m $\rightarrow$ n <= \mathrm{plus}\ m\ 1$
\end{lstlisting}
where \lstinline{<=} is an inductive type, and \lstinline{le_plus_trans} and \lstinline{plus} are both functions in the current context.
The first step wraps this in a lambda from some \lstinline{nat}, the type of \lstinline{m}:

\begin{lstlisting}[language=coq]
  $\lambda (\diff{n0} : nat) . \lambda (H : n <= m) . \mathrm{le_plus_trans}\ n\ m\ (S O)\ H)$
  $: \Pi (\diff{n0} : nat), n <= m \rightarrow n <= \mathrm{plus}\ m\ 1$
\end{lstlisting}
The second step substitutes \lstinline{n0} for \lstinline{m}:

\begin{lstlisting}[language=coq]
  $\lambda (\diff{n0} : nat) . \lambda (H : n <= \diff{n0}) . \mathrm{le_plus_trans}\ n\ \diff{n0}\ (S O)\ H)$
  $: \Pi (\diff{n0} : nat), n <= \diff{n0} \rightarrow n <= \mathrm{plus}\ \diff{n0}\ 1$
\end{lstlisting}

In general, generalization is a kind of anti-unification problem, % TODO cite and so on
as the terms and types may be reduced,
so that the common function or argument does not appear explicitly.
This is of course undecidable.
This poses a challenge for abstraction in the second step---substitution.

To handle this challenge, generalization uses a list of \textit{abstraction strategies} to determine what subterms to substitute.
In this case, the simplest strategy works: the tool
replaces all terms that are convertible to the concrete argument \lstinline{m} with the abstract argument
\lstinline{n0}, which produces a single candidate. Type checking this candidate confirms that it is a patch.
In some cases, the simplest strategy is not sufficient, even when it is possible to abstract the term.
Section~\ref{sec:pumpkin-impl-trans} describes a sample of other strategies.

It is the job of the patch finding procedure to provide the candidate and the terms to abstract.
In addition, each configuration includes a list of strategies.
The configuration for changes in conclusions, for example, starts with the simplest strategy,
and moves on to more complex strategies only if that strategy fails.
This design makes abstraction simple to extend with new strategies and simple to call with different strategies
for different configurations, or even as an optimization for the proof engineer.

\paragraph{Inversion} The tool should be able to invert a patch candidate.
This is necessary to search for isomorphisms.
It is also necessary to search for implications between propositionally
equal types, since candidates may appear in the wrong direction.
For example, consider two list lemmas (we write \lstinline{length} as \lstinline{len}):

\begin{lstlisting}[language=coq]
  old : (@\ltacforall@) l' l, len (l' ++ l) = len l' + len l(@\vspace{-0.08cm}@)
  new : (@\ltacforall@) l' l, len (l' ++ l) = len l' + len (rev l)
\end{lstlisting} 

If \sysname searches the difference in proofs of these lemmas for a patch from the 
conclusion of \lstinline{new} to the conclusion of \lstinline{old},
it may find a candidate \emph{backwards}:

\begin{lstlisting}
  candidate l' l (H : (@\diff{old}@) l' l) :=(@\vspace{-0.04cm}@)
    eq_ind_r ... (@\diff{(rev\_length l)}@)(@\vspace{-0.04cm}@)
  : (@\ltacforall@) l' l, (@\diff{old}@) l' l -> (@\diff{new}@) l' l
\end{lstlisting}
The component can invert this to get the patch: %from \lstinline{new} to \lstinline{old}:

\begin{lstlisting}
  patch l' l (H : (@\diff{new}@) l' l) :=(@\vspace{-0.04cm}@)
    eq_ind_r ... (@\diff{(eq\_sym (rev\_length l))}@)(@\vspace{-0.04cm}@)
  : (@\ltacforall@) l' l, (@\diff{new}@) l' l -> (@\diff{old}@) l' l
\end{lstlisting}
We can then use this patch to port proofs.
For example, if we add this patch to a hint database~\cite{hints},
we can port this proof:

\begin{lstlisting}
  Theorem app_rev_len : (@\ltacforall@) l l',(@\vspace{-0.04cm}@)
    len (rev (l' ++ l)) = len (rev l) + len (rev l').(@\vspace{-0.04cm}@)
  Proof.(@\vspace{-0.04cm}@)
    intros. rewrite rev_app_distr. (@\succeeds{apply old.}@)(@\vspace{-0.04cm}@)
  Qed.
\end{lstlisting}
to this proof:

\begin{lstlisting}
  Theorem app_rev_len : (@\ltacforall@) l l',(@\vspace{-0.04cm}@)
    len (rev (l' ++ l)) = len (rev l) + len (rev l').(@\vspace{-0.04cm}@)
  Proof.(@\vspace{-0.04cm}@)
    intros. rewrite rev_app_distr. (@\succeeds{apply new.}@)(@\vspace{-0.04cm}@)
  Qed.
\end{lstlisting}

Rewrites like \lstinline{candidate} are \textit{invertible}:
We can invert any rewrite in one direction by rewriting in the opposite direction.
In contrast, it is not possible to invert the patch \sysname
found for Figure~\ref{fig:example}.
Inversion will necessarily sometimes fail, since not all terms are invertible.
%Not all terms are invertible. It is not possible, for example, to invert the patch 
%Inversion will necessarily sometimes fail.

Patch inversion (\lstinline{inverting.ml}) exploits symmetry to try to reverse the conclusions of a 
candidate patch.
It first factors the candidate using the factoring component, then calls the primitive inversion
function on each factor, then finally folds the resulting list in reverse.
The primitive inversion function exploits symmetry. 
For example, equality is symmetric, so the component can invert any application of \lstinline{eq_ind} or \lstinline{eq_ind_r}
(any rewrite). Indeed, \lstinline{eq_ind} and \lstinline{eq_ind_r} are inverses, and are related by symmetry:

\begin{lstlisting}[language=coq]
  (@\diff{eq\_ind\_r}@) A x P (H : P x) y (H0 : y = x) :=(@\vspace{-0.04cm}@)
    (@\diff{eq\_ind}@) x (fun y0 : A => P y0) H y ((@\diff{eq\_sym}@) H0)	
\end{lstlisting}
If inversion does not recognize that the type is symmetric, it
swaps subterms and type-checks the result to see if it is an inverse.

\paragraph{Factoring} The tool should be able to factor a term into a sequence of lemmas.
This can help break other problems, like abstraction, into smaller subproblems.
It is also necessary to invert certain terms.
Consider inverting an arbitrary sequence of two rewrites:

\begin{lstlisting}
  t := (@\diff{eq\_ind\_r G ...}@) ((@\diff{eq\_ind\_r F ...}@))
\end{lstlisting}
We can view \lstinline{t} as a term that composes two functions:

\begin{lstlisting}
  g := (@\diff{eq\_ind\_r G ...}@)(@\vspace{-0.04cm}@)
  f := (@\diff{eq\_ind\_r F ...}@)(@\vspace{-0.04cm}@)
  t := g $\circ$ f
\end{lstlisting}
The inverse of \lstinline{t} is the following:

\begin{lstlisting}
  t$\inv$ := f$\inv$ $\circ$ g$\inv$
\end{lstlisting}
To invert \lstinline{t}, \sysname identifies the factors \lstinline{[f; g]}, 
inverts each factor to \lstinline{[f}$\inv$\lstinline{; g}$\inv$\lstinline{]}, 
then folds and applies the inverse factors in the opposite direction.

The lemma factoring component (\lstinline{factoring.ml}) searches within a term
for its factors. For example,
if the term composes two functions, it returns both factors:

\begin{lstlisting}[language=coq]
  t : (@\diff{X}@) -> (@\diff{Z}@)                (* term *)(@\vspace{-0.04cm}@)
 [f : (@\diff{X}@) -> (@\diff{Y}@); g : (@\diff{Y}@) -> (@\diff{Z}@)] (* factors *)
\end{lstlisting}
In this case, the component takes the composite term and \lstinline{X} as arguments.
It first searches as deep as possible for a term of type \lstinline{X -> Y} for some \lstinline{Y}.
If it finds such a term, then it recursively searches for a term with type \lstinline{Y -> Z}. 
It maintains all possible 
paths of factors along the way, and it discards any paths that cannot reach \lstinline{Z}.

\subsection{Limitations}
\label{sec:pumpkin-trans-limitations}

This section describes a few fundamental limitations of the transformations in \sysname,
and whether they are addressed in the later \toolnamec extension.
Limitations that are due to the choice of implementation strategy are in Section~\ref{sec:pumpkin-impl-trans}.

\paragraph{Abstraction Strategies}
Not reduced and so on.
Abstracting candidates is not always possible; abstraction will necessarily be a collection of heuristics.

\paragraph{Dependent Factoring} The current factoring algorithm can handle paths
with more than two factors, but it fails when \lstinline{Y} depends on \lstinline{X}.
Other components may benefit from dependent factoring; we leave this to future work.

\paragraph{Modeling Diverse Proof Styles} Coq programmers use diverse proof styles;
the ideal tool should support many different styles.
Proofs about decidable domains that apply the term \lstinline{dec_not_not}
pose difficulties for abstraction and inversion; the ideal tool should support these. 
\sysname has limited support for changes in hypotheses, fixpoints, constructors, 
pattern matching, and nested induction; the ideal tool should implement these features.

