\section{Testing Boundaries}
\label{sec:eval}

\begin{figure*}[ht]
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}[language=coq]
fun n m p (H : n <= m) (H0 : m <= p) =>(@\vspace{-0.04cm}@)
  (@\diff{le\_S n p}@) (* ... proof of stronger lemma *)(@\vspace{-0.04cm}@)
: (@\ltacforall@) n m p, n <= m -> m <= p -> n <= (@\diff{S p}@)
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}[language=coq]
fun n m p (H : n <= m) (H0 : m <= p) =>(@\vspace{-0.04cm}@)
  (@\diff{le\_plus\_trans n p 1}@) (* ... proof of stronger lemma *)(@\vspace{-0.04cm}@)
: (@\ltacforall@) n m p, n <= m -> m <= p -> n <= (@\diff{p + 1}@)
\end{lstlisting}
\end{minipage}
\vspace{-.35cm}
\caption{Two proof terms \lstinline{old} (left) and \lstinline{new} (right) that contain the same proof of a stronger lemma.}
\label{fig:stronger}
\end{figure*}

\lstset{language=coq, aboveskip=3pt,belowskip=3pt}

In the case studies in Section~\ref{sec:case}, we showed how
the core components are useful for real scenarios.
In this section, we explore the boundary between what the \sysname prototype can and cannot handle.
It is precisely this boundary that informs us how to improve the implementations of the
core components.

To evaluate this boundary, we tested the core components of \sysname on a suite of 50 pairs of proofs (Section~\ref{sec:suite}).
We designed 11 of these pairs to succeed, then modified their proofs to produce the remaining 39 pairs
that try to stress the core functionality of the tool.
%28 of the 50 variants did not stress \sysname at all.
We learned the following from the pairs
that tested \sysname's limitations:

\begin{enumerate}
\item \textbf{The failed pairs drive improvements.} \\
\sysname failed on 17 of 50 pairs. These pairs tell us how to improve the core. (Section~\ref{sec:fail})
\item \textbf{The pairs unearth abstraction strategies.} \\
\sysname produced an exponential number of candidates in 5 of 50 pairs.
New abstraction strategies can dramatically reduce the number of candidates. (Section~\ref{sec:fail})
\item \textbf{\sysname was fast, and it can be faster.} \\
The slowest successful patch took \SI{48}{\ms}. The slowest failure took \SI{7}{\ms}.
Simple changes can make \sysname more efficient. (Section~\ref{sec:perf})
\end{enumerate}

\subsection{Patch Generation Suite}
\label{sec:suite}

We wrote a suite\footnote{\url{http://github.com/uwplse/PUMPKIN-PATCH/blob/cpp18/plugin/coq/Variants.v}} of 50 pairs of proofs.
We wrote these proofs ourselves since searching for proof patches is a new domain,
so there was no existing benchmark suite to work with.
We used the following methodology:
%We designed 11 pairs to succeed, then modified these pairs to produce the remaining 39 pairs
%that stress the functionalti
%We designed 11 pairs to succeed:

\begin{enumerate}
\item Choose theorems \lstinline{old} and \lstinline{new}
\item Write similar inductive proofs of \lstinline{old} and \lstinline{new}
\item Modify the proof of \lstinline{old} to produce more pairs
\item Search for patches from \lstinline{new} to \lstinline{old}
\item If possible, search for patches from \lstinline{old} to \lstinline{new}
\end{enumerate}

In total, we chose 11 pairs of theorems \lstinline{old} and \lstinline{new}, and we wrote
50 pairs of proofs of those theorems.

For example, one pair of theorems \lstinline{old} and \lstinline{new} was a 
simplification of the auxiliary lemmas
that we encountered in the case study in Section~\ref{sec:foundations}.
For the first proof of \lstinline{old}, we added a rewrite, like in the case study:

\begin{lstlisting}[language=coq]
    (@\diff{rewrite <- plus\_n\_O.}@) rewrite -> plus_comm.
\end{lstlisting}

For the second proof of \lstinline{old}, we commuted the rewrites:

\begin{lstlisting}[language=coq]
    rewrite -> plus_comm. (@\diff{rewrite <- plus\_n\_O.}@)
\end{lstlisting} 

We then searched for patches in both directions,
since the conclusions of \lstinline{old}
and \lstinline{new} were propositionally equal.

Our goal was to determine what changes to proofs stress the components
and how to use that information to drive improvements.
We focused on differences in conclusions, the most supported configuration.
%The procedure defaults to \lstinline{old} when it cannot find a patch; 
%we marked each pair a success if \sysname found a direct patch,
%and a failure if it returned the default.
Since \sysname operates over terms,
we removed redundant proof terms, even if they were produced by different tactics.
We controlled the first pair of proofs of each pair of theorems for features we had not yet implemented,
like nested induction, changes in hypotheses, and abstracting \lstinline{omega} terms.
These features sometimes showed up in later proofs (for example, after moving a rewrite);
we kept these proofs in the suite, since isolated changes to supported proofs that
introduce unsupported features can inform future improvements.	

\subsection{Three Challenges}
\label{sec:fail}

\sysname found patches for 33 of the 50 pairs. 28 of the 33 successes
did not stress \sysname at all: \sysname found the correct candidate immediately and was able to abstract it
in one try.
The pairs that \sysname failed to patch and the successful pairs that stressed abstraction
reveal key information about how to improve the core components.
We walk through three examples.
Future tools can use these as challenge problems to improve upon \sysname.

\paragraph{A Challenge for Differencing} For one pair of proofs of theorems 
with propositionally equal conclusions (Figure~\ref{fig:stronger}),
the differencing component failed to find candidates in either direction.
These proofs both contain the same proof of a stronger lemma;
\sysname found patches from this lemma to
both \lstinline{old} and \lstinline{new},
but it was unable to find a patch between \lstinline{old} and \lstinline{new}.
A patch may show up deep in the difference between \lstinline{le_plus_trans}
and \lstinline{le_S}, but even if we $\delta$-reduce (unfold the definition of~\cite{equality}) \lstinline{le_plus_trans}, this is not obvious:

\begin{lstlisting}
    le_plus_trans n m p (H : n <= m) :=(@\vspace{-0.04cm}@)
      (fun lemma : m <= m + p =>(@\vspace{-0.04cm}@)
        trans_contra_inv_impl_morphism(@\vspace{-0.04cm}@)
          PreOrder_Transitive(@\vspace{-0.04cm}@)
          (m + p)(@\vspace{-0.04cm}@)
          m(@\vspace{-0.04cm}@)
          lemma)(@\vspace{-0.04cm}@)
      (le_add_r m p)(@\vspace{-0.04cm}@)
      H
\end{lstlisting}

This points to two difficulties in finding patches: Knowing when to $\delta$-reduce terms 
is difficult; exploring the appropriate time for reduction
may produce patches for pairs that \sysname currently cannot patch.
Furthermore, finding patches is more challenging
when neither theorem has a conclusion that is as strong as possible.

\paragraph{A Challenge for Inversion} For one pair of proofs with propositionally equal conclusions,
\sysname found a patch in one direction, but failed to invert it:

\begin{lstlisting}[language=coq]
    fun n m p (_ : n <= m) (_ : m <= p) (H1 : n <= p) =>(@\vspace{-0.04cm}@)
      gt_le_S n (S p) (le_lt_n_Sm n p H1)(@\vspace{-0.04cm}@)
    : (@\ltacforall@) n m p, n <= m -> m <= p -> n <= p -> S n <= S p
\end{lstlisting}

The inversion component was unable to invert this term, even though an inverse does exist.
To invert this, the component needs to know to $\delta$-reduce \lstinline{gt_le_S}:

\begin{lstlisting}[language=coq]
  gt_le_S n m :=(@\vspace{-0.04cm}@)
    (fun (H : (@\ltacforall@) n0 m0, (@\diff{n0 < m0}@) -> (@\diff{S n0 <= m0}@)) => H n m) (@\vspace{-0.1cm}@)
    ...(@\vspace{-0.1cm}@)
  : (@\ltacforall@) n m, (@\diff{n < m}@) -> (@\diff{S n <= m}@)
\end{lstlisting}

It then needs to swap the hypothesis with the conclusion in \lstinline{H} to produce the inverse:

\begin{lstlisting}[language=coq]
  gt_le_S$\inv$ n m :=(@\vspace{-0.04cm}@)
    (fun (H : (@\ltacforall@) n0 m0, (@\diff{S n0 <= m0}@) -> (@\diff{n0 < m0}@)) => H n m)(@\vspace{-0.1cm}@)
    ...(@\vspace{-0.1cm}@)
   : (@\ltacforall@) n m, (@\diff{S n <= m}@) -> (@\diff{n < m}@)
\end{lstlisting}

Inversion currently swaps subterms when it is not
aware of any symmetry properties about the inductive type. However,
it does not know when to $\delta$-reduce function definitions. Furthermore, 
there are many possible subterms to swap;
for inversion to know to only swap the subterms of \lstinline{H}, it must have a better
understanding of the structure of the term. Both of these are ways to improve inversion.

%\begin{figure*}[ht]
%\begin{minipage}{0.48\textwidth}
%\begin{lstlisting}[language=coq]
%Theorem old: (@\ltacforall@) l1 l2 : list nat,
%  length (@\diff{(rev (l1 ++ l2))}@) =
%  (length (@\diff{(rev l1)})@) + (length (@\diff{(rev l2)}@)).
%\end{lstlisting}
%\end{minipage}
%\hfill
%\begin{minipage}{0.48\textwidth}
%\begin{lstlisting}[language=coq]
%Theorem new: (@\ltacforall@) l1 l2 : list nat,
%  length (@\diff{(l1 ++ l2)}@) =
%  (length (@\diff{l1}@)) + (length (@\diff{l2}@)).
%\end{lstlisting}
%\end{minipage}
%\vspace{-.2cm}
%\caption{Two theorems \lstinline{old} (left) and \lstinline{new} (right) for which \sysname found patches in both directions.}
%\label{fig:list}
%\end{figure*}

\paragraph{A Challenge for Abstraction} Abstraction produced an exponential number of candidates when abstracting a patch candidate with this type:

\begin{lstlisting}[language=coq]
    (@\ltacforall@) n n0,(@\vspace{-0.04cm}@)
      (fun m => n <= max m n0) n ->(@\vspace{-0.04cm}@)
      (fun m => n <= max n0 m) n
\end{lstlisting}

The goal was to abstract by \lstinline{n} and produce a patch with this type:

\begin{lstlisting}[language=coq]
    (@\ltacforall@) (@\diff{m0}@) n n0,(@\vspace{-0.04cm}@)
      n <= max (@\diff{m0}@) n0 ->(@\vspace{-0.04cm}@)
      n <= max n0 (@\diff{m0}@)
\end{lstlisting}

%search found the following applied patch, For 5 variants, \sysname found a patch, but only after producing an exponential number of abstract candidates.
%While this did not have a significant impact on time, these variants uncover natural ways to extend abstraction
%to produce fewer candidates.
%For example, for one pair of proof terms, search found an applied patch:

The difficulty was in determining which occurrences of \lstinline{n} to abstract.
The component needed to abstract only the highlighted occurrences:

\begin{lstlisting}
    fun n n0 (H0 : n <= max n0 (@\diff{n}@)) =>(@\vspace{-0.04cm}@)
      @eq_ind_r (@\vspace{-0.04cm}@)
        nat (@\vspace{-0.04cm}@)
        (max n0 (@\diff{n}@))(@\vspace{-0.04cm}@)
        (fun n1 => n <= n1)(@\vspace{-0.04cm}@)
        H0(@\vspace{-0.04cm}@)
        (max (@\diff{n}@) n0)(@\vspace{-0.04cm}@)
        (max_comm (@\diff{n}@) n0)
\end{lstlisting}

The simplest abstraction strategy failed, and a more
complex strategy 
%substituted all occurrences of \lstinline{n} in the body with the abstract \lstinline{m0}.
%While this term was well-typed, it did not have the goal type.
%The next strategy substituted all combinations of occurrences of \lstinline{n} with \lstinline{m0},
%which eventually found a patch---
succeeded only after producing exponentially many candidates.
While this did not have a significant impact on time,
this case gives rise to a new class of abstraction strategies:
semantics-aware abstraction.
In this case, we know from the type of the candidate
and the type of \lstinline{eq_ind_r} that these two hypothesis types 
are equivalent (similarly for the conclusion types):

%Using the type signature of \lstinline{eq_ind_r},
%we can determine an alternative (equivalent) type for the candidate:

%\begin{lstlisting}[language=coq]
%    (@\ltacforall@) n n0,
%      (fun n1 => n <= n1) (max n0 n) ->
%      (fun n1 => n <= n1) (max n n0)
%\end{lstlisting}

%A simple semantics-aware strategy is to make sure that if we abstract any subterm, 
%we also abstract terms that depend on that subterm.
%This will dramatically reduce the number of candidates, but at the same time it does
%not require any advanced reasoning.

\begin{lstlisting}
    (fun m => n <= max m n0) n(@\vspace{-0.04cm}@)
    (fun n1 => n <= n1) (max n0 n)
\end{lstlisting}

The tool can search recursively for patches to find two patches that bridge the two equivalent
types:

\begin{lstlisting}
    p1 := fun n => max n0 n(@\vspace{-0.04cm}@)
    p2 := fun n => max n n0
\end{lstlisting}

Then the candidate type is exactly this:

\begin{lstlisting}
    (@\ltacforall@) n n0,(@\vspace{-0.04cm}@)
      (fun n1 => n <= n1) (@\diff{(p2 n)}@) ->(@\vspace{-0.04cm}@)
      (fun n1 => n <= n1) (@\diff{(p1 n)}@)
\end{lstlisting}

Abstraction should thus abstract the highlighted subterms and the
terms that have types constrained by those subterms.
%It should not change \lstinline{(fun n1 => n <= n1)}.
This produces a patch in one candidate:

\begin{lstlisting}
    fun (@\diff{m0}@) n n0 (H0 : n <= max n0 (@\diff{m0}@)) =>(@\vspace{-0.04cm}@)
      @eq_ind_r (@\vspace{-0.04cm}@)
        nat (@\vspace{-0.04cm}@)
        (max n0 (@\diff{m0}@))           (* p1 (@\diff{m0}@) *)(@\vspace{-0.04cm}@)
        (fun n1 => n <= n1) (* P *)(@\vspace{-0.04cm}@)
        H0                    (* : P (p1 (@\diff{m0}@)) *)(@\vspace{-0.04cm}@)
        (max (@\diff{m0}@) n0)           (* p2 (@\diff{m0}@) *)(@\vspace{-0.04cm}@)
        (max_comm (@\diff{m0}@) n0)      (* : p1 (@\diff{m0}@) = p2 (@\diff{m0}@) *)
\end{lstlisting}

%This is a significant engineering effort, but it generalizes beyond this case.
This same strategy would find a patch for one of the pairs that \sysname failed to 
abstract. %This suggests that even if this strategy increases runtime,
%it 
This is a natural future direction for abstraction.

\subsection{Performance}
\label{sec:perf}

\sysname performed well for all pairs. The slowest success took \SI{48}{\ms}.\footnote{i7-4790K, at 4.00 GHz, 32 GB RAM}
When \sysname failed, it failed fast. The slowest failure took \SI{7}{\ms}.
While we find this promising, proof terms were small ($\le$ 67 LOC);
we leave evaluating performance on larger terms
to future work.

%When \sysname failed, it failed fast. The slowest failure took \SI{7}{\ms}.
%This was a case of nested induction, which we are yet to implement: \sysname interpreted the difference of parameters
%in the nested inductive case as significant and tried to abstract that candidate. This difference
%was not actually significant, so search failed.

% TODO double-check that that's true

\sysname was slowest %at finding patches 
when the patch showed up inverted in the difference of proofs,
since \sysname had to search twice, once in each direction.
A future procedure may %implement search in both directions at once,
%or may 
determine which direction to search first ahead of time; proof term size may be a simple heuristic for this.

%; we suspect this will impose a slight overhead in the easy
%direction, but reduce the overhead in the difficult direction. Alternatively, future procedures may determine


%first searches for a patch from \lstinline{old} to \lstinline{new}, and if that fails,
%only then does it search backwards for a patch from \lstinline{new} to \lstinline{old}. If it finds a 
%an inverted patch, it then attempts to invert it.
%This means that it searches twice, once in each direction, and then uses the lemma factoring and inversion components.

%For example, \sysname was able to patch two proofs of the theorems \lstinline{old} and \lstinline{new} in Figure~\ref{fig:list} in %both directions.
%However, \sysname was much faster in finding the patches from \lstinline{new} to \lstinline{old} than it
%was at finding patches from \lstinline{old} to \lstinline{new}: It took
%\SI{2}{\ms} to patch the first variant and \SI{10}{\ms} to patch the second variant, whereas in the opposite direction, it
%took \SI{4}{\ms} to patch the first variant and \SI{18}{\ms} to patch the second variant.

%It makes sense that \sysname was slower in searching from \lstinline{old} to \lstinline{new}: The current algorithm
%first searches for a patch from \lstinline{old} to \lstinline{new}, and if that fails,
%only then does it search backwards for a patch from \lstinline{new} to \lstinline{old}. If it finds a 
%an inverted patch, it then attempts to invert it.
%This means that it searches twice, once in each direction, and then uses the lemma factoring and inversion components.

%Future algorithms may implement search in both directions at once; we suspect this will impose a slight overhead in the easy
%direction, but reduce the overhead in the difficult direction. Alternatively, future algorithms may determine
%sswhich direction to search first ahead of time; proof term size may be a simple heuristic for this.





