\section{Differencing}
\label{sec:pi-diff}

% TODO explain how this fits into differencing, that differencing can be done by the human or by the tool, and so on

\begin{figure*}
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
DepConstr(0, list T) : list T := Constr((@\codediff{0}@), list T).(@\vspace{-0.04cm}@)
DepConstr(1, list T) t l : list T :=(@\vspace{-0.04cm}@)
  Constr ((@\codediff{1}@), list T) t l.(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
DepElim(l, P) { p$_{\mathtt{nil}}$, p$_{\mathtt{cons}}$ } : P l :=(@\vspace{-0.04cm}@)
  Elim(l, P) { (@\codediff{p$_{\mathtt{nil}}$}@), (@\codediff{p$_{\mathtt{cons}}$}@) }.(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.48\textwidth}
\begin{lstlisting}
DepConstr(0, list T) : list T := Constr((@\codediff{1}@), list T).(@\vspace{-0.04cm}@)
DepConstr(1, list T) t l : list T :=(@\vspace{-0.04cm}@)
  Constr((@\codediff{0}@), list T) t l.(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
DepElim(l, P) { p$_{\mathtt{nil}}$, p$_{\mathtt{cons}}$ } : P l :=(@\vspace{-0.04cm}@)
  Elim(l, P) { (@\codediff{p$_{\mathtt{cons}}$}@), (@\codediff{p$_{\mathtt{nil}}$}@) }.(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\vspace{-0.3cm}
\caption{The dependent constructors and eliminators for old (left) and new (right) \lstinline{list}, with the difference in \codediff{orange}.}
\vspace{-0.1cm}
\label{fig:listconfig}
\end{figure*}

Differencing---whether done by the tool (automatic configuration) or by the proof engineer (manual configuration)---identifies 
and proves a type equivalence. But differening further decomposes that equivalence into a form called a \textit{configuration}. 
The configuration is the key to building a proof term transformation that implements transport in a way that is suitable for repair.

Each configuration corresponds to an equivalence \Aa $\simeq$ \B.
It deconstructs the equivalence into things that talk about \Aa, and things that talk about \B.
It does so in a way that hides details
specific to the equivalence, like the order or number of arguments to an induction principle or type.

At a high level, the configuration helps the transformation achieve two goals: 

\begin{enumerate}
\item preserve equality up to transport across the equivalence between \Aa and \B (Section~\ref{sec:pi-diff-equiv}), and 
\item produce well-typed terms (Section~\ref{sec:pi-diff-equal}).
\end{enumerate}
To differencing, this configuration is a pair of pairs of terms:

\begin{lstlisting}
  ((DepConstr, DepElim), (Eta, Iota))(@\vspace{-0.05cm}@)
\end{lstlisting}
each of which corresponds to one of the two goals:
\lstinline{DepConstr} and \lstinline{DepElim} define how to transform constructors and eliminators, thereby preserving the equivalence, and 
\lstinline{Eta} and \lstinline{Iota} define how to transform $\eta$-expansion and $\iota$-reduction of constructors and eliminators, thereby producing well-typed terms.

Formally, every correct configuration corresponds to an initial algebra of an endofunctor,
and correctness---that every configurations and equivalences are isomorphic---follows by Lambek's theorem (Section~\ref{sec:art}).\footnote{When I met with Michael Shulman over coffee in San Diego a few years ago, he at various points said that a preliminary version of this work ``feels like univalence,'' ``feels like coherence,'' and ``feels like an endofunctor.'' All three were correct! But I didn't understand the connection to endofunctors by way of Lambek's until a few months before writing this thesis.
Anders M\"{o}rtberg and Carlo Angiuli identified this connection, and Carlo explained it to me in great detail.
It is quite beautiful!}  % TODO
Each search procedure for automatic configuration produces both the configuration and the equivalence that it induces (Section~\ref{sec:proc}).
Manual configuration takes as input the configuration directly.
(Mention Section~\ref{sec:pi-diff-limits} somehow.)

All terms that I introduce in this section are in \kl{CIC$_{\omega}$} with \kl{primitive eliminators}. % TODO point to other conventions again
Section~\ref{sec:pi-diff-limits} describes the limitations, and Section~\ref{sec:pi-implementation} describes how I scale this from CIC$_{\omega}$ to Coq.

\subsection{Preserving the Equivalence}
\label{sec:pi-diff-equiv}

To preserve the equivalence, the configuration maps terms over \Aa to terms over \B by viewing each
term of type \B as if it were an \Aa.
This way, the transformation in Section~\ref{sec:pi-trans} can replace values of \Aa with values of \B, and
inductive proofs about \Aa with inductive proofs about \B, %, then recursively transform
%subterms 
all without changing the order or number of arguments.

The two configuration parts responsible for this are \lstinline{DepConstr}
and \lstinline{DepElim} (\textit{dependent constructors} and \textit{eliminators}).
These describe how to construct and eliminate \Aa and \B, wrapping the types with a common inductive structure.
The transformation requires the same number of dependent constructors and cases in dependent eliminators for \Aa and \B,
even if \Aa and \B are types with different numbers of constructors
(\Aa and \B need not even be inductive; see Sections~\ref{sec:art} and~\ref{sec:pi-results}).

For the \lstinline{list} change from Section~\ref{sec:overview},
the configuration that \toolnamec discovers uses the dependent constructors
and eliminators in Figure~\ref{fig:listconfig}. The dependent constructors for \lstinline{Old.list}
are the normal constructors with the order unchanged,
while the dependent constructors for \lstinline{New.list} swap constructors
back to the original order.
Similarly, the dependent eliminator for \lstinline{Old.list} is the normal eliminator for \lstinline{Old.list},
while the dependent eliminator for \lstinline{New.list} swaps cases.

As the name hints, these constructors and eliminators can be dependent.
Consider the type of vectors of some length:

\begin{lstlisting}
packed_vect T := $\Sigma$(n : nat).vector T n.(@\vspace{-0.05cm}@)
\end{lstlisting}
\toolnamec can port proofs across the equivalence between this type and \lstinline{list T}~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/examples/Example.v}{\circled{3}}. % Example.v
The dependent constructors \toolnamec discovers pack the index into an existential, like:

\begin{lstlisting}
DepConstr(0, packed_vect) : packed_vect T :=(@\vspace{-0.04cm}@)
  $\exists$ (Constr(0, nat)) (Constr(0, vector T)).(@\vspace{-0.05cm}@)
\end{lstlisting}
and the eliminator it discovers eliminates the projections:

\begin{lstlisting}
DepElim(s, P) { f$_0$ f$_1$ } : P ($\exists$ ($\pi_l$ s) ($\pi_r$ s)) :=(@\vspace{-0.04cm}@)
  Elim($\pi_r$ s, $\lambda$(n : nat)(v : vector T n).P ($\exists$ n v)) {(@\vspace{-0.04cm}@)
    f$_0$,(@\vspace{-0.04cm}@)
    ($\lambda$(t : T)(n : nat)(v : vector T n).f$_1$ t ($\exists$ n v))(@\vspace{-0.04cm}@)
  }.(@\vspace{-0.05cm}@) 
\end{lstlisting}

In both these examples, the interesting work moves into the configuration:
the configuration for the first swaps constructors and cases,
and the configuration for the second maps constructors and cases over \lstinline{list} to constructors and cases over \lstinline{packed_vect}. %packs constructors and eliminates projections.
That way, the transformation need not add, drop, or reorder arguments---it can truly be generic over type equivalences.
%In essence, all of the difficult work moves into the configuration.
Furthermore, both examples use automatic configuration, so differencing in \toolnamec's \textbf{Configure} component
discovers \lstinline{DepConstr} and \lstinline{DepElim} from just the types \Aa and \B, taking care of even the difficult work.

\subsection{Producing Well-Typed Terms}
\label{sec:pi-diff-equal}

The other configuration parts \lstinline{Eta} and \lstinline{Iota} deal with producing well-typed terms,
in particular by transporting equalities.
CIC$_{\omega}$ distinguishes between two important kinds of equality: those that hold by reduction (\textit{definitional} equality), and those that hold by proof (\textit{propositional} equality).
That is, two terms \lstinline{t} and \lstinline{t'} of type \lstinline{T} are definitionally equal if they reduce to the same normal form,
and propositionally equal if there is a proof that \lstinline{t = t'} using the inductive
equality type \lstinline{=} at type \lstinline{T}. Definitionally equal terms are necessarily propositionally equal, but 
the converse is not in general true. % TODO this should probably go into an earlier section

When a datatype changes, sometimes, definitional equalities defined over the old version of that type must become propositional.
A naive proof term transformation may fail to generate well-typed terms if it does not account for this.
Otherwise, if the transformation transforms a term \lstinline{t : T} to some \lstinline{t' : T'}, it does not necessarily
transform \lstinline{T} to \lstinline{T'}~\cite{tabareau2019marriage}.

\lstinline{Eta} and \lstinline{Iota} describe how to transport equalities.
More formally, they define $\eta$-expansion and $\iota$-reduction of \Aa and \B,
which may be propositional rather than definitional,
and so must be explicit in the transformation.
$\eta$-expansion describes how to expand a term to apply a constructor to an eliminator in a way that preserves propositional equality,
and is important for defining dependent eliminators~\cite{nlab:eta-conversion}.
$\iota$-reduction ($\beta$-reduction for inductive types) describes how to reduce an elimination of a constructor~\cite{nlab:beta-reduction}.

The configuration for the change from \lstinline{list} to \lstinline{packed_vect} has propositional \lstinline{Eta}.
It uses $\eta$-expansion for $\Sigma$:

\begin{lstlisting}
  Eta(packed_vect) := $\lambda$(s:packed_vect).$\exists$ ($\pi_l$ s) ($\pi_r$ s).(@\vspace{-0.05cm}@)
\end{lstlisting}
which is propositional and not definitional in Coq.
Thanks to this, we can forego the assumption that our language has primitive projections (definitional $\eta$ for $\Sigma$).

\begin{figure}
\begin{minipage}{0.44\columnwidth}
   \lstinputlisting[firstline=1, lastline=8]{often/nattobin.tex}
\end{minipage}
\hfill
\begin{minipage}{0.54\columnwidth}
   \lstinputlisting[firstline=10, lastline=17]{often/nattobin.tex}
\end{minipage}
\vspace{-0.2cm}
\caption{A unary natural number \lstinline{nat} (left) is either zero (\lstinline{0}) or the successor of some other natural number (\lstinline{S}).
A binary natural number \lstinline{N} (right) is either zero (\lstinline{N0}) or a positive binary number (\lstinline{Npos}), where a positive binary number is either 1 (\lstinline{xH}), or the result of shifting left and adding 1 (\lstinline{xI}) or
0 (\lstinline{xO}). Unary and binary natural numbers are equivalent, but have different inductive structures.
Consequentially, definitional equalities over \lstinline{nat} may become propositional over \lstinline{N}.}
\vspace{-0.2cm}
\label{fig:nattobin}
\end{figure}

Each \lstinline{Iota}---one per constructor---describes and proves the $\iota$-reduction behavior
of \lstinline{DepElim} on the corresponding case.
This is needed, for example, to port proofs about unary numbers \lstinline{nat} to
proofs about binary numbers \lstinline{N} (Figure~\ref{fig:nattobin}).
While we can define \lstinline{DepConstr} and \lstinline{DepElim} to induce an equivalence
between them~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/nonorn.v}{\circled{5}}, % nonorn.v (TODO update links throughout)
we run into trouble reasoning about applications of \lstinline{DepElim},
since proofs about \lstinline{nat} that hold by reflexivity do not necessarily hold by reflexivity over \lstinline{N}. 
For example, in Coq, while \lstinline{S (n + m)  = S n + m} holds by reflexivity over \lstinline{nat},
when we define \lstinline{+} with \lstinline{DepElim} over \lstinline{N},
the corresponding theorem over \lstinline{N} does not hold by reflexivity.

To transform proofs about \lstinline{nat} to proofs about \lstinline{N}, we must transform \textit{definitional} $\iota$-reduction over \lstinline{nat} to \textit{propositional} $\iota$-reduction over \lstinline{N}.
For our choice of \lstinline{DepConstr} and \lstinline{DepElim},
$\iota$-reduction is definitional over \lstinline{nat}, since a proof of:

\begin{lstlisting}
  $\forall$ P p$_\texttt{0}$ p$_\texttt{S}$ n,(@\vspace{-0.04cm}@)
    DepElim((@\codediff{DepConstr(1, nat) n}@), P) { p$_\texttt{0}$, p$_\texttt{S}$ } =(@\vspace{-0.04cm}@)
    (@\codediff{p$_\texttt{S}$}@) n (DepElim(n, P) { p$_\texttt{0}$, p$_\texttt{S}$ }).(@\vspace{-0.05cm}@)
\end{lstlisting}
holds by reflexivity.
\lstinline{Iota} for \lstinline{nat} in the \lstinline{S} case is a rewrite by that proof by reflexivity~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/nonorn.v}{\circled{5}},
with type:

\begin{lstlisting}
  $\forall$ P p$_\texttt{0}$ p$_\texttt{S}$ n (Q: P (DepConstr(1, nat) n) $\rightarrow$ s),(@\vspace{-0.04cm}@)
    Iota(1, nat, Q) :(@\vspace{-0.04cm}@)
      Q ((@\codediff{p$_\texttt{S}$}@) n (DepElim(n, P) { p$_\texttt{0}$, p$_\texttt{S}$ })) $\rightarrow$(@\vspace{-0.04cm}@)
      Q (DepElim((@\codediff{DepConstr(1, nat) n}@), P) { p$_\texttt{0}$, p$_\texttt{S}$ }).(@\vspace{-0.05cm}@)
\end{lstlisting}
In contrast, $\iota$ for \lstinline{N} is propositional, since the 
theorem: %over \lstinline{N}:

\begin{lstlisting}
  $\forall$ P p$_\texttt{0}$ p$_\texttt{S}$ n,(@\vspace{-0.04cm}@)
    DepElim((@\codediff{DepConstr(1, N) n}@), P) { p$_\texttt{0}$, p$_\texttt{S}$ } =(@\vspace{-0.04cm}@)
    (@\codediff{p$_\texttt{S}$}@) n (DepElim(n, P) { p$_\texttt{0}$, p$_\texttt{S}$ }).(@\vspace{-0.05cm}@)
\end{lstlisting}
no longer holds by reflexivity.
\lstinline{Iota} for \lstinline{N} is a rewrite by the propositional equality that proves this theorem~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/nonorn.v}{\circled{5}},
with type:

\begin{lstlisting}
  $\forall$ P p$_\texttt{0}$ p$_\texttt{S}$ n (Q: P (DepConstr(1, N) n) $\rightarrow$ s),(@\vspace{-0.04cm}@)
    Iota(1, N, Q) :(@\vspace{-0.04cm}@)
      Q ((@\codediff{p$_\texttt{S}$}@) n (DepElim(n, P) { p$_\texttt{0}$, p$_\texttt{S}$ })) $\rightarrow$(@\vspace{-0.04cm}@)
      Q (DepElim((@\codediff{DepConstr(1, N) n}@), P) { p$_\texttt{0}$, p$_\texttt{S}$ }).(@\vspace{-0.05cm}@)
\end{lstlisting}
By replacing \lstinline{Iota} over \lstinline{nat} with \lstinline{Iota} over \lstinline{N},
the transformation replaces rewrites by reflexivity over \lstinline{nat} to rewrites by propositional equalities over \lstinline{N}.
That way, \lstinline{DepElim} behaves the same over \lstinline{nat} and \lstinline{N}.

Taken together over both \Aa and \B, \lstinline{Iota} describes how the inductive structures of \Aa and \B differ.
The transformation requires that \lstinline{DepElim} over \Aa and over \B have the same structure
as each other, so if \Aa and \B \textit{themselves} have the same 
inductive structure (if they are \textit{ornaments}~\cite{mcbride}),
then if $\iota$ is definitional for \Aa, it will be possible to choose
\lstinline{DepElim} with definitional $\iota$ for \B.
Otherwise, if \Aa and \B (like \lstinline{nat} and \lstinline{N}) have different inductive structures,
then definitional $\iota$ over one would become propositional $\iota$ over the other.

\subsection{Configurations as Initial Algebras}
\label{sec:art}

\begin{figure*}
\begin{minipage}{0.43\textwidth}
\begin{lstlisting}
section: $\forall$ (a : A), g (f a) = a.(@\vspace{-0.04cm}@)
retraction: $\forall$ (b : B), f (g b) = b.(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
constr_ok: $\forall$ $j$ $\vec{x_A}$ $\vec{x_B}$, $\vec{x_A}$ $\equiv_{A \simeq B}$ $\vec{x_B}$ $\rightarrow$(@\vspace{-0.04cm}@)
  DepConstr($j$, A) $\vec{x_A}$ $\equiv_{A \simeq B}$ DepConstr(j, B) $\vec{x_B}$.(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
elim_ok: $\forall$ a b P$_A$ P$_B$ $\vec{f_A}$ $\vec{f_B}$,(@\vspace{-0.04cm}@)
  a $\equiv_{A \simeq B}$ b $\rightarrow$(@\vspace{-0.04cm}@)
  P$_A$ $\equiv_{(A \rightarrow s) \simeq (B \rightarrow s)}$ P$_B$ $\rightarrow$(@\vspace{-0.04cm}@)
  $\forall$ $j$, $\vec{f_A}$[j] $\equiv_{\xi (A, P_A, j) \simeq \xi (B, P_B, j)}$ $\vec{f_B}$[j]$\rightarrow$(@\vspace{-0.04cm}@)
  DepElim(a, P$_A$) $\vec{f_A}$ $\equiv_{(P a) \simeq (P b)}$ DepElim(b, P$_B$) $\vec{f_A}$.(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\hfill
\begin{minipage}{0.56\textwidth}
\begin{lstlisting}
elim_eta(A): $\forall$ a P $\vec{f}$, DepElim(a, P) $\vec{f}$ : P (Eta(A) a).(@\vspace{-0.04cm}@)
eta_ok(A): $\forall$ (a : A), Eta(A) a = a.(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
(@\phantom{constr\_ok: $\forall$ $j$ $\vec{x_A}$ $\vec{x_B}$,}@)(@\vspace{-0.04cm}@)
(@\phantom{  DepConstr($j$, A) $\vec{x_A}$ $\equiv_{A \simeq B}$ DepConstr(j, B) $\vec{x_B}$.}@)(@\vspace{-0.04cm}@)
(@\vspace{-0.14cm}@)
iota_ok(A): $\forall$ $j$ P $\vec{f}$ $\vec{x}$ (Q: P(Eta(A) (DepConstr($j$, A) $\vec{x}$)) $\rightarrow$ s),(@\vspace{-0.04cm}@)
  Iota(A, j, Q) : (@\vspace{-0.04cm}@)
    Q (DepElim(DepConstr(j, A) $\vec{x}$, P) $\vec{f}$) $\rightarrow$ (@\vspace{-0.04cm}@)
    Q (rew $\leftarrow$ eta_ok(A) (DepConstr(j, A) $\vec{x}$) in(@\vspace{-0.04cm}@)
      ($\vec{f}$[j]$\ldots$(DepElim(IH$_0$, P) $\vec{f}$)$\ldots$(DepElim(IH$_n$, P) $\vec{f}$)$\ldots$)).(@\vspace{-0.04cm}@)
\end{lstlisting}
\end{minipage}
\vspace{-0.2cm}
\caption{Correctness criteria for a configuration to ensure that the transformation
preserves equivalence (left) coherently with equality (right, shown for \Aa; \B is similar). \lstinline{f} and \lstinline{g} are defined in text. $s$, $\vec{f}$, $\vec{x}$, and $\vec{\mathtt{IH}}$ represent
sorts, eliminator cases, constructor arguments, and inductive hypotheses. $\xi$ $(A,$ $P,$ $j)$ is the type 
of \lstinline{DepElim(A, P)} at \lstinline{DepConstr(j, A)} (similarly for \B).} %, respectively.}
\label{fig:spec}
\end{figure*}

(Carlo theory will be integrated into here when I have time: basically the names aren't coincidences, it's because this corresponds
to an initial algebra, so it's more natural when you have inductive types but more general than that.
Draw diagram, explain what each part corresponds to. Explain that correctness is just Lambek's!)

Choosing a configuration necessarily depends in some way on the proof engineer's intentions:
there can be infinitely many equivalences that correspond to a 
change, only some of which are useful (for example~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/playground/refine_unit.v}{\circled{7}}, any \Aa is equivalent to \lstinline{unit} refined by \Aa). % refine_unit.v
And there can be many configurations that correspond
to an equivalence, some of which will produce terms that are more useful or efficient than others
(consider \lstinline{DepElim} converting through several intermediate types).

While we cannot control for intentions, we \textit{can} specify what it means for a chosen configuration to be correct:
Fix a configuration. Let \lstinline{f} be the function that uses \lstinline{DepElim} to eliminate \Aa and \lstinline{DepConstr} to construct \B,
and let \lstinline{g} be similar.
Figure~\ref{fig:spec} specifies the correctness criteria for the configuration.
These criteria relate \lstinline{DepConstr}, \lstinline{DepElim}, \lstinline{Eta}, and \lstinline{Iota}
in a way that preserves equivalence coherently with equality.

\paragraph{Equivalence}
To preserve the equivalence (Figure~\ref{fig:spec}, left), \lstinline{DepConstr} and \lstinline{DepElim} must form an equivalence
(\lstinline{section} and \lstinline{retraction} must hold for \lstinline{f} and \lstinline{g}).
\lstinline{DepConstr} over \Aa and \B must be equal up to transport across that equivalence (\lstinline{constr_ok}), 
and similarly for \lstinline{DepElim} (\lstinline{elim_ok}).
Intuitively, \lstinline{constr_ok} and \lstinline{elim_ok} guarantee that the transformation
correctly transports dependent constructors and dependent eliminators,
as doing so will preserve equality up to transport for those subterms.
This makes it possible for the transformation
to avoid applying \lstinline{f} and \lstinline{g}, instead porting terms from \Aa directly to \B.

\paragraph{Equality}
To ensure coherence with equality (Figure~\ref{fig:spec}, right),
\lstinline{Eta} and \lstinline{Iota} must prove $\eta$ and $\iota$.
That is, \lstinline{Eta} must have the same definitional behavior as the dependent eliminator (\lstinline{elim_eta}),
and must behave like identity (\lstinline{eta_ok}).
Each \lstinline{Iota} must prove and rewrite along the simplification (\textit{refolding}~\cite{boutillier:tel-01054723}) behavior that corresponds to a case of the dependent eliminator (\lstinline{iota_ok}).
This makes it possible for the transformation to
avoid applying \lstinline{section} and \lstinline{retraction}.

\paragraph{Correctness}
With these correctness criteria for a configuration, we get the completeness result (proven in Coq~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/playground/arbitrary.v}{\circled{8}}) that every equivalence induces a configuration. % arbitrary.v
We also obtain an algorithm for the soundness result that every configuration induces an equivalence.

The algorithm to prove \lstinline{section} is as follows (\lstinline{retraction} is similar):
replace \lstinline{a} with \lstinline{Eta(A) a} by \lstinline{eta_ok(A)}.
Then, induct using \lstinline{DepElim} over \Aa.
For each case $i$, the proof obligation is to show that \lstinline{g (f a)} is equal to \lstinline{a},
where \lstinline{a} is \lstinline{DepConstr(A, i)} applied to the non-inductive arguments (by \lstinline{elim_eta(A)}).
Expand the right-hand side using \lstinline{Iota(A, i)}, then expand it again using \lstinline{Iota(B, i)}
(destructing over each \lstinline{eta_ok} to apply the corresponding \lstinline{Iota}).
The result follows by definition of \lstinline{g} and \lstinline{f}, and by reflexivity.

\paragraph{Equivalences from Configurations}
The algorithm above is essentially what differencing uses for each search procedure to generate functions \lstinline{f} and \lstinline{g} for the automatic configurations~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/search/search.ml}{\circled{9}}, % search.ml
and also generate proofs \lstinline{section} and \lstinline{retraction} that these functions form an equivalence~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/search/equivalence.ml}{\circled{10}}. % equivalence.ml
To minimize dependencies, \toolnamec does not produce proofs of \lstinline{constr_ok} and \lstinline{elim_ok} directly,
as stating these theorems cleanly would require either a special framework~\cite{tabareau2017equivalences}
or a univalent type theory~\cite{univalent2013homotopy}.
If the proof engineer wishes, it is possible to prove these in individual cases~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/coq/playground/arbitrary.v}{\circled{8}}, % arbitray.v
but this is not necessary in order to use \toolnamec. %---they simply need to hold.
% TODO not quite true

\subsection{Search Procedures}
\label{sec:proc}

\toolnamec implements four search procedures for automatic configuration~\href{https://github.com/uwplse/pumpkin-pi/blob/v2.0.0/plugin/src/automation/lift/liftconfig.ml}{\circled{6}}:

\begin{enumerate}
\item algebraic ornaments,
\item unpacking $\Sigma$ types,
\item swapping constructors, and
\item moving between nested pairs and records.
\end{enumerate}
I detail the first of these in Section~\ref{sec:alg-orn},
then briefly describe how the rest differ in Section~\ref{sec:other}.
I explain how these are implemented in Section~\ref{sec:pi-implementation}.

% DEVOID 3.1 unchanged (needs to talk about configuration now, and the A and B thing will be confusing here, and needs refactoring and merging and so on)

\subsection{Algebraic Ornaments}
\label{sec:alg-orn}

% TODO somewhere, mention how to unpack sigma types

The first search procedure discovers equivalences that correspond to \intro{algebraic ornaments}.
An algebraic ornament relates an inductive type to an indexed version of that type,
where the new index is fully determined by a unique fold over \Aa. 
For example, \lstinline{vector} is exactly \lstinline{list} with a new index of type \lstinline{nat},
where the new index is fully determined by the \lstinline{length} function (recall Figure~\ref{fig:listtovect} on page~\pageref{fig:listtovect}).
The equivalence that we already saw in Section~\ref{sec:pi-scope} follows from this:

\begin{lstlisting}
  $\Sigma$(l : list T).length l = n $\equiv$ vector T n
\end{lstlisting}
Alternatively, we can say that a list is equivalent to a vector of \textit{some} length:

\begin{lstlisting}
  list T $\equiv$ $\Sigma$(n : nat).vector T n
\end{lstlisting}
As usual, this equivalence is made up of two functions \lstinline{f} and \lstinline{g}, along with proofs \lstinline{section} and \lstinline{retraction}.
In addition, for algebraic ornaments, there is a proof of this theorem:

\begin{lstlisting}
  $\forall$ (l : list T), length l = $\pi_{l}$ (f l)
\end{lstlisting}
which states that the \lstinline{length} function is coherent with this equivalence.

In Section~\ref{sec:pi-results}, I will show you a case study of porting functions and proofs from lists to length-indexed vectors.
Nominally this works by porting the functions and proofs along the equivalence from Section~\ref{sec:pi-scope},
but in practice this works by chaining two different automatic configurations with some human input.
The first configuration uses a search procedure that discovers the equivalence between lists and vectors of some length above,
as well as the proof of coherence.
The second configuration uses a search procedure that discovers how to unpack vectors of some length to vectors of a \textit{particular} length.

The first configuration nicely demonstrates how differencing works, so let us look at this in detail.
Assume the existence of inductive types \Aa and \AI, related by an algebraic ornament with the index of type \I.
In the scope of this thesis, further assume that \Aa and \I are not indexed types.\footnote{The original paper that this is from lets all of \Aa,
\AI, and \I be \textit{indexed} inductive types, with the new index of type \I appearing anywhere within the list of indices of \AI;
the implementation makes the same decision.
I felt that this was very important to show in detail when I wrote that paper, since indices are often omitted,
even though handling them is one of the trickiest parts of implementing an algorithm like this.
In this thesis, however, I decided to simplify the presentation and assume the types are not indexed,
and so the new index is the only index of \AI.
I do recommend checking out the original paper if you would really like to implement something like this over indexed types---it is formalized
those who are \textit{sufficiently fanatical}.}
Then there is a type equivalence:

\begin{lstlisting}
  $A\ \simeq\ \Sigma (i : I).A_I\ i$
\end{lstlisting}
In addition, there is an indexer, which is a unique fold:

\begin{lstlisting}
  indexer : $A\ \rightarrow\ I$.
\end{lstlisting}
which projects the promoted index:
\begin{lstlisting}
  coherence : $\forall (a : A),\ $indexer$\ a = \pi_{l}\ ($f$\ a)$.
\end{lstlisting}
Following existing work, I call this equivalence the \textit{ornamental promotion isomorphism}~\cite{ko2016programming}; 
when it holds and the indexer exists, I say that \AI is an algebraic ornament of \Aa.

In their original form, ornaments are a programming mechanism: given a type \Aa, an ornament determines
some new type \AI. Differencing inverts this process for algebraic ornaments: given types \Aa\ and \AI, 
it searches for the configuration that induces the ornamental promotion isomorphism between them.
This is possible for algebraic ornaments precisely because the indexer is extensionally unique.
For example, all possible indexers for \lstinline{list} and \lstinline{vector} must compute
the length of a list; if we were to try doubling the length instead, we would not be able to satisfy the equivalence.

% DEVOID 4.1 mostly unchanged

\subparagraph*{Common Definitions.}
The algorithm assumes a function \lstinline{new} that determines whether a hypothesis in a case of the eliminator type of \AI\ is new.
Figure~\ref{fig:common} contains other common definitions, the names for which are reserved:
Input type \Aa\ expands to an inductive type with constructors
\smallmath{$\{\mathrm{C}_{A_1}, \ldots, \mathrm{C}_{A_n}\}$}.
\smallmath{$\mathrm{P}_A$} denotes the type of the motive of the eliminator of \Aa,
and each \smallmath{$\mathrm{E}_{A_j}$} denotes the type of the eliminator for the $j$th constructor of \Aa.
Analogous names are also reserved for input type \AI.

\begin{figure}
\begin{lstlisting}
$A$ := $\mathrm{Ind} (\mathit{Ty}_A : \mathrm{s}_A)\{\mathrm{C}_{A_1}, \ldots, \mathrm{C}_{A_n}\}$
$A_I$ := $\mathrm{Ind} (\mathit{Ty}_{A_I} : (\Pi (i : I). \mathrm{s}_{A_I}))\{\mathrm{C}_{A_{I_1}}, \ldots, \mathrm{C}_{A_{I_n}}\}$

$\mathrm{P}_A$ := $\Pi (a : A) . \mathrm{s}_A$
$\mathrm{P}_{A_I}$ := $\Pi (i : I) (a_i : A_I\ i) . \mathrm{s}_{A_I}$

$\forall 1 \le j \le n,$
  $\mathrm{E}_{A_j}\ (p_A : \mathrm{P}_A)$ := $\xi(A,\ p_A,\ \mathrm{Constr}(j,\ A),\ C_{A_j})$
  $\mathrm{E}_{B_j}\ (p_{A_I} : \mathrm{P}_{A_I})$ := $\xi(A_I,\ p_{A_I},\ \mathrm{Constr}(j,\ A_I),\ C_{A_{I_j}})$
\end{lstlisting}
\vspace{-0.3cm}
\caption{Common definitions.}
\label{fig:common}
\end{figure}

% TODO note code produces equivalence directly for historical reasons I guess? Maybe in implementation section?
The differencing starts by generating the \lstinline{indexer} (Figure~\ref{fig:searchindexer}),
then uses that to generate each of the configuration parts (Figure~\ref{searchconfig}).

\paragraph{Differencing for the Indexer}
Then differencing generates the \lstinline{indexer} by traversing the types of the eliminators for \Aa\ and \AI\ in parallel using the algorithm from Figure~\ref{fig:searchindexer},
which consists of three judgments: one to generate the motive, one to generate each case,
and one to compose the motive and cases.

\begin{figure}
\begin{mathpar}
%\begin{minipage}{0.45\textwidth}
\mprset{flushleft} 
\small
\hfill\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i_{m}} t$ }\vspace{-0.55cm}\\

\inferrule[Index-Motive] 
  { \\ }
  { \Gamma \vdash (A, A_I) \Downarrow_{i_{m}} \lambda (a : A) . I }\\

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i_{c}} t$ }\vspace{-0.4cm}\\

\inferrule[Index-Conclusion]
  { \\ }
  { \Gamma \vdash (p_A\ a,\ p_{A_I}\ i\ a_i) \Downarrow_{i_{c}} i } 

\inferrule[Index-Hypothesis] % new hypothesis for index
  { \mathrm{new}\ n_{A_I}\ b_{A_I} \\\\ \Gamma,\ n_{A_I} : t_{A_I} \vdash (\Pi (n_A : t_A) . b_A,\ b_{A_I}) \Downarrow_{i_{c}} t }
  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_{A_I} : t_{A_I}) . b_{A_I}) \Downarrow_{i_{c}} t}

\inferrule[Index-IH] % inductive hypothesis
  { \Gamma \vdash (A, A_I) \Downarrow_{i_{m}} p \\
    \Gamma,\ n_A : p\ a \vdash (b_A,\ b_{A_I} [n_A / i]) \Downarrow_{i_{c}} t }
  { \Gamma \vdash (\Pi (n_A : p_A\ a) . b_A,\ \Pi (n_{A_I} : p_{A_I}\ i\ b) . b_{A_I}) \Downarrow_{i_{c}} \lambda (n_A : p\ a) . t }

\inferrule[Index-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
  { \Gamma,\ n_A : t_A \vdash (b_A,\ b_{A_I} [n_A / n_{A_I}]) \Downarrow_{i_{c}} t }
  { \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_{A_I} : t_{A_I}) . b_{A_I}) \Downarrow_{i_{c}} \lambda (n_A : t_A) . t }\\

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i} t$ }\vspace{-0.5cm}\\

\inferrule[Index-Ind] 
  { \Gamma \vdash (A,\ A_I) \Downarrow_{i_{m}} p \\
    \Gamma,\ p_A : \mathrm{P}_A,\ p_{A_I} : \mathrm{P}_{A_I} \vdash \{ (\mathrm{E}_{A_1}\ p_A,\ \mathrm{E}_{A_{I_1}}\ p_{A_I}),\ldots,(\mathrm{E}_{A_n}\ p_A,\ \mathrm{E}_{A_{I_n}}\ p_{A_I}) \} \Downarrow_{i_{c}} \vec{f} } 
  { \Gamma \vdash (A,\ A_I) \Downarrow_{i} \lambda (a : A) . \mathrm{Elim}(a, p) \vec{f}}
\end{mathpar}	
\vspace{-0.5cm}
\caption{Differencing for the indexer function.}
\label{fig:searchindexer}
\end{figure}

\subparagraph*{Generating the Motive.}
The \smallmath{$(T_A,\ T_{A_I}) \Downarrow_{i_{m}} t$} judgment consists of only the derivation \textsc{Index-Motive},
which computes the indexer motive from the types \Aa\ and \AI\ (expanded in Figure~\ref{fig:common}).
It does this by constructing a function from \Aa to \I.
Consider \lstinline{list} and \lstinline{vector}:
\begin{lstlisting}
  list T := Ind (Ty$_A$ : Type) {$\ldots$} $\phantom{sep}$ vector T := Ind (Ty$_B$ : $\Pi$(@\codediff{(n : nat)}@).Type) {$\ldots$}
\end{lstlisting}
For these types, \textsc{Index-Motive} computes the motive:
\begin{lstlisting}
  (@\codeauto{$\lambda$ (l : list T) . nat}@)
\end{lstlisting} 

\subparagraph*{Generating Each Case.}
The \smallmath{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i_{c}} t$} judgment generates each case of the indexer
by traversing in parallel the corresponding cases of the eliminator types for \Aa\ and \AI.
It consists of four derivations:
\textsc{Index-Conclusion} handles base cases and conclusions of inductive cases,
while \textsc{Index-Hypothesis}, \textsc{Index-IH}, and \textsc{Index-Prod} recurse into
products.

\textsc{Index-Hypothesis} handles each new hypothesis that corresponds to a new index in an inductive hypothesis
of an inductive case of the eliminator type for \AI. It adds the new index to the environment, then recurses into the body of only the
type for which the index already exists. For example, in the inductive case of \lstinline{list} and \lstinline{vector},
\lstinline{new} determines that \lstinline{n} is the new hypothesis.
\textsc{Index-Hypothesis} then recurses into the body of only the \lstinline{vector} case:
\begin{lstlisting}
  $\Pi$ (t$_l$:T) (l : list T) (IH$_l$ : p$_A$ l), $\ldots$ $\phantom{sep}$ $\Pi$ (t$_v$ : T) (v : vector T n) (IH$_v$ : p$_{A_I}$ (@\codediff{n}@) v), $\ldots$
\end{lstlisting}
\textsc{Index-Prod} is next. It recurses into product types when the hypothesis is neither a new index nor an inductive hypothesis. Here, it runs twice, recursing into the body and substituting names %appropriately
until it hits the inductive hypothesis for both types:
\begin{lstlisting}
  $\Pi$ (IH$_l$ : p$_A$ l), p$_A$ (cons t$_l$ l) $\phantom{separation}$ $\Pi$ (IH$_v$ : p$_{A_I}$ (@\codediff{n}@) l),  p$_{A_I}$ (@\codediff{(S n)}@) (consV (@\codediff{n}@) t$_l$ l)
\end{lstlisting}
\textsc{Index-IH} then takes over. It substitutes the new motive in the inductive hypothesis, then recurses into both bodies, 
substituting the new inductive hypothesis for the index in the eliminator type for \AI.
Here, it substitutes the new motive
for \smallmath{$\mathrm{p}_A$} in the type of \lstinline{IH$_l$}, extends the environment with \lstinline{IH$_l$}, then 
substitutes \lstinline{IH$_l$} for \lstinline{n}, so that it recurses on these types:
\begin{lstlisting}
  p$_A$ (cons t$_l$ l) $\phantom{separationseparationsepara}$ p$_{A_I}$ (@\codediff{(S IH$_l$)}@) (consV (@\codediff{IH$_l$}@) t$_l$ l)
\end{lstlisting}
Finally, \textsc{Index-Conclusion} computes the conclusion by taking the new index of the application of the motive \smallmath{$p_{A_I}$},
here \lstinline{S IH$_l$}.
In total, this produces a function 
that computes the length of \lstinline{cons t l}:
\begin{lstlisting}
  (@\codeauto{$\lambda$ (t$_l$ : T) (l : list T) (IH$_l$ : ($\lambda$ (l : list T) . nat) l) . S IH$_l$}@)
\end{lstlisting}

\subparagraph*{Composing the Result.}
The \smallmath{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i} t$} judgment consists of only \textsc{Index-Ind}, which 
identifies the motive and each case using the other two judgments, then composes the result. In the case of \lstinline{list} and \lstinline{vector},
this produces a function that computes the length of a list:
\begin{lstlisting}
  (@\codeauto{$\lambda$ (l : list T).}@)(@\codeauto{Elim(l, $\lambda$ (l : list T) . nat)}@) 
    (@\codeauto{\{0, $\lambda$ (t$_l$ : T) (l : list T) (IH$_l$ : ($\lambda$ (l : list T) . nat) l) . S IH$_l$\}}@)
\end{lstlisting}

\begin{figure}
\begin{mathpar}
\mprset{flushleft}  
\small
\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(t_A,\ t_{A_I}) \Downarrow_{D_C} (t'_A,\ t'_{A_I}) $ }\\

\inferrule[DepConstr] 
  { \Gamma \vdash (A,\ A_I) \Downarrow_{i} \pi } 
  { \Gamma \vdash (\mathrm{Constr}(j,\ A),\ \mathrm{Constr} (j,\ A_I)) \Downarrow_{D_C}
                  (\mathrm{Constr}(j,\ A),\ \lambda (a : A) . \exists\ (\pi\ a)\ (\mathrm{Constr}(j,\ A_I) \ldots)) }

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{D_E} \ldots $ }\\

\inferrule[DepElim] 
  { \\ } 
  { \\ }

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{\eta} \vec{t}$ }\\

\inferrule[Eta] 
  { \\ } 
  { \\ }

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{\iota} \{ (t_{A_1}, t_{A_{I_1}}) \ldots (t_{A_n}, t_{A_{I_n}}) \} $ }\\

\inferrule[Iota] 
  { \\ } 
  { \\ }

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_c ((\mathrm{DepConstr}, \mathrm{DepElim}), (\mathrm{Eta}, \mathrm{Iota}))$ }\\

\inferrule[Config] 
  { \Gamma \vdash \{ (\mathrm{Constr}(1,\ A),\ \mathrm{Constr}(1,\ A_I)) \ldots (\mathrm{Constr}(n,\ A),\ \mathrm{Constr}(n,\ A_I)) \} \Downarrow_{D_C} \{ (\mathrm{DepConstr}(1,\ A), \mathrm{DepConstr}(1,\ A_I)) \ldots (\mathrm{DepConstr}(n,\ A), \mathrm{DepConstr}(n,\ A_I)) \} \\
    \Gamma \vdash (A,\ A_I) \Downarrow_{D_E} D_E \\\\
    \Gamma \vdash (A,\ A_I) \Downarrow_{\eta} \eta \\
    \Gamma \vdash (A,\ A_I) \Downarrow_{\iota} \vec{\iota} }
  { \Gamma \vdash (A,\ A_I) \Downarrow_{c} ((\{ (\mathrm{DepConstr}(1,\ A), \mathrm{DepConstr}(1,\ A_I)) \ldots (\mathrm{DepConstr}(n,\ A), \mathrm{DepConstr}(n,\ A_I)) \}, \vec{D_E}), (\eta, \vec{\iota}))}
\end{mathpar}	
\vspace{-0.5cm}
\caption{Differencing for the configuration. (Work in progress! Porting from an old form.)}
\label{fig:searchconfig}
\end{figure}

\paragraph{Differencing for the Configuration}
Figure~\ref{fig:searchconfig} shows the derivations for the judgment \smallmath{$(T_A,\ T_{A_I}) \Downarrow_c c$}
that differences the two types for the configuration.
This consists of four judgments: one for each configuration part.

\subparagraph*{Differencing for \lstinline{DepConstr}.} (Work in progress.)

\subparagraph*{Differencing for \lstinline{DepElim}.} (Work in progress.)

\subparagraph*{Differencing for \lstinline{Eta}.} (Work in progress.)

\subparagraph*{Differencing for \lstinline{Iota}.} (Work in progress.)

\subparagraph*{Composing the Result.}
The core differencing algorithm produces the configuration parts, then composes them into a tuple.
In addition, it saves \lstinline{indexer}, and uses the algorithm shown earlier to discover
\lstinline{f} and \lstinline{g} from the configuration, and prove \lstinline{section} and \lstinline{retraction}
hold for \lstinline{f} and \lstinline{g}.
For algebraic ornaments, it additional proves coherence by reflexivity.

\iffalse
that searches for \lstinline{promote}:
\textsc{Promote-Motive} identifies the motive 
as \B\ with a new index (which it computes using \lstinline{indexer}, denoted by metavariable \smallmath{$\pi$}).
When \textsc{Promote-IH} recurses, it substitutes the inductive hypothesis for the term rather than
for its index, and it substitutes the new index (which it also computes using \lstinline{indexer}) inside of that term.
\textsc{Promote-Conclusion} returns the entire term, rather than its index.
Finally, \textsc{Promote-Ind} not only recurses into each case, but also packs the result.

%The differencing algorithm is in Figure~\ref{searchconfig}.
%It builds on five intermediate steps: one to generate \lstinline{indexer} (Figure~\ref{fig:searchindexer}),
%and one to generate each of the configuration parts (Figures~\ref{searchconstr}, \ref{searchelim}, \ref{searcheta}, and~\ref{searchiota}).

\begin{figure}
\begin{mathpar}
\mprset{flushleft}  
\small
\hfill\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_B) \Downarrow_{p_{m}} t$ }\vspace{-0.55cm}\\

\inferrule[Promote-Motive]
  { \Gamma \vdash (A,\ B) \Downarrow_{i} \pi }
  { \Gamma \vdash (A,\ B) \Downarrow_{p_{m}} \lambda (\vec{i_a} : \vec{\mathrm{X}_A}) (a : A\ \vec{i_a}) . B\ (\mathrm{index}\ (\pi\ \vec{i_a}\ a)\ \vec{i_a}) }\\

\hfill\phantom{sorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysor}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_B) \Downarrow_{p_{c}} t$ }\vspace{-0.4cm}\\

\inferrule[Promote-Conclusion]
  { \\ }
  { \Gamma \vdash (p_A\ \vec{i_A}\ a,\ p_B\ \vec{i_B}\ b) \Downarrow_{p_{c}} b }

\inferrule[Promote-IH] % inductive hypothesis
  { \Gamma \vdash (A,\ B) \Downarrow_{i} \pi \\
    \Gamma \vdash (A,\ B) \Downarrow_{p_{m}} p \\\\
    \Gamma,\ n_A : p\ \vec{i_A}\ a \vdash (b_A,\ b_B [n_A / b] [\pi\ \vec{i_A}\ a / \vec{i_B}[\mathrm{off}\ A\ B]]) \Downarrow_{p_{c}} t }
  { \Gamma \vdash (\Pi (n_A : p_A\ \vec{i_A}\ a) . b_A,\ \Pi (n_B : p_B\ \vec{i_B}\ b) . b_B) \\\\
    \phantom{\Gamma} \Downarrow_{p_{c}}  \lambda (n_A : p\ \vec{i_A}\ a) . t }

\hfill\phantom{sorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysorrysor}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_B) \Downarrow_{p} t$ }\vspace{-0.5cm}\\

\inferrule[Promote-Ind] 
  { \Gamma \vdash (A,\ B) \Downarrow_{i} \pi \\
    \Gamma \vdash (A,\ B) \Downarrow_{p_{m}} p \\\\
    \Gamma,\ p_A : \mathrm{P}_A,\ p_B : \mathrm{P}_B \vdash \{ (\mathrm{E}_{A_1}\ p_A,\ \mathrm{E}_{B_1}\ p_B),\ldots,(\mathrm{E}_{A_n}\ p_A,\ \mathrm{E}_{B_n}\ p_B) \} \Downarrow_{p_{c}} \vec{f} } 
  { \Gamma \vdash (A,\ B) \Downarrow_{p} \lambda (\vec{i_A} : \vec{\mathrm{X}_A}) (a : A\ \vec{i_A}) . \exists\ (\pi\ \vec{i_A}\ a)\ (\mathrm{Elim}(a, p) \vec{f})}
\end{mathpar}	
\vspace{-0.5cm}
\caption{Identifying the promotion function.}
\label{fig:searchpromote}
\end{figure}

\begin{figure}
\begin{mathpar}
%\begin{minipage}{0.45\textwidth}
\mprset{flushleft} 
\small
\hfill\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i_{m}} t$ }\vspace{-0.55cm}\\

\inferrule[Index-Motive] 
  { \\ }
  { \Gamma \vdash (A, A_I) \Downarrow_{i_{m}} \lambda (a : A) . I }\\

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i_{c}} t$ }\vspace{-0.4cm}\\

\inferrule[Index-Conclusion]
  { \\ }
  { \Gamma \vdash (p_A\ a,\ p_{A_I}\ i\ a_i) \Downarrow_{i_{c}} i } 

\inferrule[Index-Hypothesis] % new hypothesis for index
  { \mathrm{new}\ n_{A_I}\ b_{A_I} \\\\ \Gamma,\ n_{A_I} : t_{A_I} \vdash (\Pi (n_A : t_A) . b_A,\ b_{A_I}) \Downarrow_{i_{c}} t }
  {  \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_{A_I} : t_{A_I}) . b_{A_I}) \Downarrow_{i_{c}} t}

\inferrule[Index-IH] % inductive hypothesis
  { \Gamma \vdash (A, A_I) \Downarrow_{i_{m}} p \\
    \Gamma,\ n_A : p\ a \vdash (b_A,\ b_{A_I} [n_A / i]) \Downarrow_{i_{c}} t }
  { \Gamma \vdash (\Pi (n_A : p_A\ a) . b_A,\ \Pi (n_{A_I} : p_{A_I}\ i\ b) . b_{A_I}) \Downarrow_{i_{c}} \lambda (n_A : p\ a) . t }

\inferrule[Index-Prod] % otherwise, unchanged (when we get rid of the gross fall-through thing, needs not new, and needs to check t_A and t_B not IHs)
  { \Gamma,\ n_A : t_A \vdash (b_A,\ b_{A_I} [n_A / n_{A_I}]) \Downarrow_{i_{c}} t }
  { \Gamma \vdash (\Pi (n_A : t_A) . b_A,\ \Pi (n_{A_I} : t_{A_I}) . b_{A_I}) \Downarrow_{i_{c}} \lambda (n_A : t_A) . t }\\

\hfill\phantom{woooooooooooooooooooooooooooooooooooooooooooooooo}\fbox{$\Gamma$ $\vdash$ $(T_A,\ T_{A_I}) \Downarrow_{i} t$ }\vspace{-0.5cm}\\

\inferrule[Index-Ind] 
  { \Gamma \vdash (A,\ A_I) \Downarrow_{i_{m}} p \\
    \Gamma,\ p_A : \mathrm{P}_A,\ p_{A_I} : \mathrm{P}_{A_I} \vdash \{ (\mathrm{E}_{A_1}\ p_A,\ \mathrm{E}_{A_{I_1}}\ p_{A_I}),\ldots,(\mathrm{E}_{A_n}\ p_A,\ \mathrm{E}_{A_{I_n}}\ p_{A_I}) \} \Downarrow_{i_{c}} \vec{f} } 
  { \Gamma \vdash (A,\ A_I) \Downarrow_{i} \lambda (a : A) . \mathrm{Elim}(a, p) \vec{f}}
\end{mathpar}	
\vspace{-0.5cm}
\caption{Differencing for the indexer function.}
\label{fig:searchindexer}
\end{figure}

The omitted derivations to search for \lstinline{forget} are similar,
except that the domain and range are switched. Consequentially, \lstinline{indexer} is never needed;
\textsc{Forget-Motive} removes the index rather than inserting it, and \textsc{Forget-IH} no longer substitutes the index.
Additionally, \textsc{Forget-Hypothesis} adds the hypothesis for the new index
rather than skipping it, and \textsc{Forget-Ind} eliminates over the projection rather than packing the result. %of applying the eliminator.
\fi

\subsubsection{Other Search Procedures}
\label{sec:other}

(Brief explanation of these and how differencing works for them in detail based on algebraic ornaments
Designing new Search Procedures:
How hard, how useful.)

\subsection{Limitations}
\label{sec:pi-diff-limits}

(Limitations and whether they're addressed in other tools yet or not.)

\iffalse
\paragraph{Syntactic Restrictions} Basically syntactic restrictions on inputs for automatic differencing, documented in \toolnamec.

\begin{itemize}
\item \textbf{Inputs}: Inductive types \Aa\ and \B, assuming:
\begin{itemize}
\item \B\ is an algebraic ornament of \Aa,
\item \B\ has the same number of constructors in the same order as \Aa,
\item \Aa\ and \B\ do not contain recursive references to themselves under products, and
\item for every recursive reference to \Aa\ in \Aa, there is exactly one new hypothesis in \B, which is exactly the new index of the corresponding recursive reference in \B.
\end{itemize}
\item \textbf{Outputs}: Functions \lstinline{promote}, \lstinline{forget}, and \lstinline{indexer}, guaranteeing:
\begin{itemize}
\item the outputs form the ornamental promotion isomorphism between the inputs.
\end{itemize}
\end{itemize}
\fi

